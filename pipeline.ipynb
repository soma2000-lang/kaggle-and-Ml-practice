{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":8218776,"sourceType":"datasetVersion","datasetId":4871830},{"sourceId":9851004,"sourceType":"datasetVersion","datasetId":6044627},{"sourceId":9916435,"sourceType":"datasetVersion","datasetId":6093908},{"sourceId":9976695,"sourceType":"datasetVersion","datasetId":6138580},{"sourceId":9981326,"sourceType":"datasetVersion","datasetId":6141925},{"sourceId":9992860,"sourceType":"datasetVersion","datasetId":6150200},{"sourceId":10122395,"sourceType":"datasetVersion","datasetId":6246161},{"sourceId":10129723,"sourceType":"datasetVersion","datasetId":6251458},{"sourceId":10182450,"sourceType":"datasetVersion","datasetId":4581967},{"sourceId":200567623,"sourceType":"kernelVersion"},{"sourceId":202903897,"sourceType":"kernelVersion"},{"sourceId":210357319,"sourceType":"kernelVersion"},{"sourceId":180631,"sourceType":"modelInstanceVersion","modelInstanceId":153926,"modelId":176386},{"sourceId":187838,"sourceType":"modelInstanceVersion","modelInstanceId":160135,"modelId":182502},{"sourceId":189477,"sourceType":"modelInstanceVersion","modelInstanceId":161545,"modelId":183906},{"sourceId":189794,"sourceType":"modelInstanceVersion","modelInstanceId":161818,"modelId":183919},{"sourceId":190807,"sourceType":"modelInstanceVersion","modelInstanceId":162643,"modelId":184989},{"sourceId":191552,"sourceType":"modelInstanceVersion","modelInstanceId":163276,"modelId":185622},{"sourceId":191622,"sourceType":"modelInstanceVersion","modelInstanceId":163332,"modelId":185687}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"%env N_EX=4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T05:40:09.920489Z","iopub.execute_input":"2024-12-13T05:40:09.920726Z","iopub.status.idle":"2024-12-13T05:40:09.929450Z","shell.execute_reply.started":"2024-12-13T05:40:09.920700Z","shell.execute_reply":"2024-12-13T05:40:09.928620Z"}},"outputs":[{"name":"stdout","text":"env: N_EX=4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Stage 1: Retriever\n\nRetrieve candidate misconceptions","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Retriever Scripts","metadata":{}},{"cell_type":"code","source":"%%writefile eedi_llm_retriever.py\n\nimport sys\n\nsys.path.insert(0, '/kaggle/input/eedi-utils-v04')\n\nimport os\nimport argparse\nimport gc\nimport json\nimport os\nimport warnings\nfrom copy import deepcopy\nfrom itertools import chain\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom accelerate import Accelerator\n\nfrom llm_embedding.eedi_dataset import MathDataset\nfrom llm_embedding.eedi_loader import TextCollator\nfrom llm_embedding.eedi_model import BiEncoderModel\n\nfrom utils.retriever_utils import semantic_search\n\nfrom omegaconf import OmegaConf\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nimport torch.distributed as dist\n\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import AutoConfig, AutoModel, BitsAndBytesConfig\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef print_line(): print(\"--\"*40)\n\ndef query_formatting_func(query):\n    task_description = \"\"\"Retrieve the key misconception behind the wrong answer when given a math problem and its incorrect and correct solutions.\"\"\"\n    return f\"Instruct: {task_description}\\nQuery: {query}\"\n\n\ndef get_base_model(cfg):\n    config = AutoConfig.from_pretrained(cfg.model.backbone_path, trust_remote_code=False)\n    config.use_cache = False\n\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16,\n    )\n    \n    model = AutoModel.from_pretrained(\n        cfg.model.backbone_path, \n        config=config, \n        quantization_config=bnb_config,\n        attn_implementation=cfg.model.attn_implementation, \n        low_cpu_mem_usage=True\n    )\n        \n    model.config.pretraining_tp = 1\n    return model\n    \n# ---\ndef show_batch(batch, tokenizer, n_examples=4, print_fn=print):\n    bs = batch[\"input_ids\"].size(0)\n    print_fn(f\"batch size: {bs}\")\n\n    print_fn(f\"shape of input_ids: {batch['input_ids'].shape}\")\n\n    print(\"--\" * 80)\n    for idx in range(n_examples):\n        print_fn(f\"[Text]:\\n{tokenizer.decode(batch['input_ids'][idx], skip_special_tokens=False)}\")\n        print_fn(\"--\" * 80)\n    \ndef eedi_process_df(df):\n    df = deepcopy(df)\n    grouped = df.groupby(\"QuestionId\")\n\n    question_dict = {}\n    for question_id, group in grouped:\n        question_data = group.to_dict(orient=\"records\")[0]\n        del question_data[\"QuestionId\"]\n        question_dict[question_id] = question_data\n\n    all_questions = list(question_dict.keys())\n\n    queries = []\n\n    # ---\n    for qid in all_questions:\n        info = question_dict[qid]\n\n        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n            if info[\"CorrectAnswer\"] == answer_key:\n                continue\n            \n            this_example = dict()\n            this_key = f\"{qid}_{answer_key}\"\n            this_example[\"query_id\"] = this_key\n            \n            # ---\n            for col in [\"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n                this_example[col] = info[col]\n\n            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n            queries.append(this_example)\n    # --\n    query_df = pd.DataFrame(queries)\n    return query_df\n\n\ndef add_retrieved_results(cfg, infer_df, content_df):\n    \"\"\"\n    find top-k similar chunks for each question\n    \"\"\"\n    # accelerator -----------------------------------------------------------------------#\n    accelerator = Accelerator()\n\n    query_df = eedi_process_df(infer_df)\n\n    # queries ---------------------------------------------------------------------------#\n    ds_handle = MathDataset(cfg, query_formatting_func=query_formatting_func)\n    tokenizer = ds_handle.tokenizer\n    \n    query_ds = ds_handle.get_dataset(query_df, is_query=True)\n    content_ds = ds_handle.get_dataset(content_df, is_query=False)\n    \n    query_ds = query_ds.sort(\"input_length\")\n    content_ds = content_ds.sort(\"input_length\")\n    \n    query_ids = query_ds[\"query_id\"]\n    content_ids = content_ds[\"content_id\"]\n\n    collator = TextCollator(tokenizer=tokenizer)\n\n    query_dl = DataLoader(\n        query_ds,\n        batch_size=cfg.predict_params.query_bs,\n        shuffle=False,\n        collate_fn=collator,\n    )\n\n    content_dl = DataLoader(\n        content_ds,\n        batch_size=cfg.predict_params.content_bs,\n        shuffle=False,\n        collate_fn=collator,\n    )\n    \n    # show ---\n    accelerator.print(\"Showing a batch (Query)...\")\n    for b in query_dl:\n        show_batch(b, tokenizer, print_fn=accelerator.print)\n        break\n        \n    accelerator.print(\"Showing a batch (Content)...\")\n    for b in content_dl:\n        show_batch(b, tokenizer, print_fn=accelerator.print)\n        break\n    \n    # model -----------------------------------------------------------------------------#\n    base_model = get_base_model(cfg)\n    model = BiEncoderModel(cfg, base_model, accelerator)\n    \n    # prepare ---------------------------------------------------------------------------#\n    model, query_dl, content_dl = accelerator.prepare(model, query_dl, content_dl)\n\n    # query embeddings ------------------------------------------------------------------#\n    query_embeddings = []\n    progress_bar = tqdm(range(len(query_dl)))\n\n    for batch in query_dl:\n        with torch.no_grad():\n            batch_query_embeddings = accelerator.unwrap_model(model).encode(batch)\n        batch_query_embeddings = accelerator.gather_for_metrics(batch_query_embeddings)\n        query_embeddings.append(batch_query_embeddings)\n        progress_bar.update(1)\n    progress_bar.close()\n\n    query_embeddings = torch.cat(query_embeddings, dim=0)\n    accelerator.print(f\"shape of query embeddings: {query_embeddings.shape}\")\n    assert query_embeddings.shape[0] == len(query_ids)\n    \n    # content embeddings ----------------------------------------------------------------#\n    content_embeddings = []\n    progress_bar = tqdm(range(len(content_dl)))\n\n    for batch in content_dl:\n        with torch.no_grad():\n            batch_content_embeddings = accelerator.unwrap_model(model).encode(batch)\n        batch_content_embeddings = accelerator.gather_for_metrics(batch_content_embeddings)\n        content_embeddings.append(batch_content_embeddings)\n        progress_bar.update(1)\n    progress_bar.close()\n\n    content_embeddings = torch.cat(content_embeddings, dim=0)\n    accelerator.print(f\"shape of content embeddings: {content_embeddings.shape}\")\n    assert content_embeddings.shape[0] == len(content_ids)\n\n    # top-k search ----------------------------------------------------------------------#\n    results = semantic_search(query_embeddings, content_embeddings, top_k = cfg.model.n_neighbour)\n    \n    pred_content_ids, pred_scores = [], []\n    for idx, re_i in enumerate(results):\n        query_id = query_ids[idx]\n        hit_i = [node[\"corpus_id\"] for node in re_i]\n        top_scores_i = [node[\"score\"] for node in re_i]\n        top_content_ids_i = [content_ids[pos] for pos in hit_i]\n        pred_content_ids.append(top_content_ids_i)\n        pred_scores.append(top_scores_i)\n    \n    result_df = pd.DataFrame()\n    result_df[\"query_id\"] = query_ids\n    result_df[\"pred_ids\"] = pred_content_ids\n    result_df[\"pred_scores\"] = pred_scores\n    \n    # get oof df\n    oof_df = result_df.copy()\n    oof_df = oof_df.rename(columns={\"query_id\": \"QuestionId_Answer\"})\n    oof_df = oof_df.rename(columns={\"pred_ids\": \"MisconceptionId\"})\n    oof_df[\"MisconceptionId\"] = oof_df[\"MisconceptionId\"].apply(lambda x: list(map(str, x)))\n\n    print_line()\n    accelerator.print(\"Sample Prediction:\")\n    accelerator.print(oof_df.sample().T)\n    print_line()\n    \n    return oof_df\n\n# ------\n\n\ndef execute_inference(cfg, save_dir, model_name):\n    test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n    \n    if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        n_ex = int(os.getenv(\"N_EX\"))\n        test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").head(n_ex)\n    content_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n    content_df = content_df.rename(columns={\"MisconceptionId\": \"content_id\"})\n        \n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        test_df = add_retrieved_results(cfg, test_df, content_df)\n    elif cfg.run_on_save:\n        test_df = add_retrieved_results(cfg, test_df, content_df)\n    else:\n        test_df = pd.read_parquet(\"./retriever_outputs/intfloat.parquet\")\n\n    save_path = os.path.join(save_dir, f\"{model_name}.parquet\")\n    test_df.to_parquet(save_path)\n    \n    if dist.is_initialized():\n        dist.destroy_process_group()\n\nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument('--config_path', type=str, required=True)\n    ap.add_argument('--save_dir', type=str, required=True)\n    ap.add_argument('--model_name', type=str, required=True)\n\n    args = ap.parse_args()\n    cfg = OmegaConf.load(args.config_path)\n\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    # execution\n    execute_inference(cfg, save_dir=args.save_dir, model_name=args.model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T05:40:16.262058Z","iopub.execute_input":"2024-12-13T05:40:16.262363Z","iopub.status.idle":"2024-12-13T05:40:16.271432Z","shell.execute_reply.started":"2024-12-13T05:40:16.262336Z","shell.execute_reply":"2024-12-13T05:40:16.270523Z"}},"outputs":[{"name":"stdout","text":"Overwriting eedi_llm_retriever.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 1.2 Retriever Configs","metadata":{}},{"cell_type":"code","source":"%%writefile eedi_retriever_intfloat.yaml\nrun_on_save: true\n\nmodel:\n    backbone_path: /kaggle/input/eedi-embed-intfloat-cv476-ff-4bit\n    max_length: 768\n    sentence_pooling_method: last\n    attn_implementation: eager\n    negatives_cross_device: false\n    add_eos_token: true\n    padding_side: left\n    trust_remote_code: false\n\n    n_neighbour: 128\n\npredict_params:\n    query_bs: 8\n    content_bs: 8\n\ntrain_params: # fix, should not need these for infer\n    sub_batch_size: 8 \n    num_hard_negatives: 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:20:34.683651Z","iopub.execute_input":"2024-12-07T21:20:34.683978Z","iopub.status.idle":"2024-12-07T21:20:34.689644Z","shell.execute_reply.started":"2024-12-07T21:20:34.683947Z","shell.execute_reply":"2024-12-07T21:20:34.688673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile eedi_retriever_qwen.yaml\nrun_on_save: false\nmodel:\n    backbone_path: /kaggle/input/eedi-embed-qwen14b-cv486-ff-4bit\n    max_length: 768\n    sentence_pooling_method: last\n    attn_implementation: eager\n    negatives_cross_device: false\n    add_eos_token: true\n    padding_side: left\n    trust_remote_code: false\n\n    n_neighbour: 128\n\npredict_params:\n    query_bs: 8\n    content_bs: 8\n\ntrain_params:\n    sub_batch_size: 8 # fix\n    num_hard_negatives: 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:20:34.83999Z","iopub.execute_input":"2024-12-07T21:20:34.840501Z","iopub.status.idle":"2024-12-07T21:20:34.845636Z","shell.execute_reply.started":"2024-12-07T21:20:34.840474Z","shell.execute_reply":"2024-12-07T21:20:34.844657Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.3 Retriever Inference","metadata":{}},{"cell_type":"code","source":"%%time\n!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 eedi_llm_retriever.py \\\n--config_path ./eedi_retriever_intfloat.yaml \\\n--save_dir ./retriever_outputs \\\n--model_name intfloat","metadata":{"trusted":true,"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-07T21:20:35.142881Z","iopub.execute_input":"2024-12-07T21:20:35.143497Z","iopub.status.idle":"2024-12-07T21:22:48.752772Z","shell.execute_reply.started":"2024-12-07T21:20:35.143427Z","shell.execute_reply":"2024-12-07T21:22:48.751512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 eedi_llm_retriever.py \\\n--config_path ./eedi_retriever_qwen.yaml \\\n--save_dir ./retriever_outputs \\\n--model_name qwen_14b","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:22:48.754896Z","iopub.execute_input":"2024-12-07T21:22:48.755209Z","iopub.status.idle":"2024-12-07T21:23:03.608255Z","shell.execute_reply.started":"2024-12-07T21:22:48.755176Z","shell.execute_reply":"2024-12-07T21:23:03.607216Z"},"scrolled":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.4 Prepare for Ranker\n- Prepares input for re-ranking (\"./retriever_outputs/ranker_input.parquet\")\n- Prepares blended predictions from retrievers (\"./retriever_outputs/stage_one_blended.parquet\")","metadata":{}},{"cell_type":"code","source":"%%writefile prepare_for_ranker.py\n\nimport argparse\nimport os\nfrom copy import deepcopy\n\nimport pandas as pd\nfrom omegaconf import OmegaConf\n\n\ndef process_df(df):\n    df = deepcopy(df)\n    grouped = df.groupby(\"QuestionId\")\n\n    question_dict = {}\n    for question_id, group in grouped:\n        question_data = group.to_dict(orient=\"records\")[0]\n        del question_data[\"QuestionId\"]\n        question_dict[question_id] = question_data\n\n    all_questions = list(question_dict.keys())\n\n    queries = []\n\n    for qid in all_questions:\n        info = question_dict[qid]\n\n        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n            if info[\"CorrectAnswer\"] == answer_key:\n                continue\n            this_example = dict()\n            this_key = f\"{qid}_{answer_key}\"\n            this_example[\"QuestionId_Answer\"] = this_key\n\n            # ---\n            for col in [\"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n                this_example[col] = info[col]\n\n            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n            this_example[\"AllOptionText\"] = \"\\n- \".join([info[f\"Answer{x}Text\"] for x in [\"A\", \"B\", \"C\", \"D\"]])\n            this_example[\"AllOptionText\"] = f\"\\n- {this_example['AllOptionText']}\"\n            queries.append(this_example)\n\n    query_df = pd.DataFrame(queries)\n    return query_df\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config-path\", type=str)\n    args = parser.parse_args()\n\n    with open(args.config_path, \"r\") as f:\n        cfg = OmegaConf.load(f)\n\n    os.makedirs(cfg.output_dir, exist_ok=True)\n\n    # load data ---\n    test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n        n_ex = int(os.getenv(\"N_EX\"))\n        test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").head(n_ex)\n    concept_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n    concept_df[\"MisconceptionId\"] = concept_df[\"MisconceptionId\"].astype(str)\n\n    # load retriever outputs ---\n    flat_dfs = []\n    for fp in cfg.retriever_outputs:\n        df = pd.read_parquet(fp)\n        print(f\"Loading from {fp}\")\n        print(df.head(1).T)\n        print(\"--\"*40)\n        flat_df = df.explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n        flat_dfs.append(flat_df)\n\n    # blend retriever scores ---\n    ret_preds = pd.concat(flat_dfs, ignore_index=True)\n    ret_preds = ret_preds.groupby([\"QuestionId_Answer\", \"MisconceptionId\"])[\"pred_scores\"].agg(\"sum\").reset_index()\n    ret_preds[\"pred_scores\"] = ret_preds[\"pred_scores\"] / len(cfg.retriever_outputs)\n\n    # re-sort by pred_scores\n    grouped = ret_preds.groupby(\"QuestionId_Answer\")\n\n    results = []\n    for question_id, group in grouped:\n        sorted_group = group.sort_values(\"pred_scores\", ascending=False)\n        result = {\"QuestionId_Answer\": question_id, \"MisconceptionId\": list(sorted_group[\"MisconceptionId\"]), \"pred_scores\": list(sorted_group[\"pred_scores\"])}\n        results.append(result)\n    ret_preds = pd.DataFrame(results)\n\n    # save for later use ---\n    print(\"--\" * 40)\n    save_path = os.path.join(cfg.output_dir, cfg.blned_file_name)\n    print(f\"Saving stage one blended predictions to {save_path}\")\n    ret_preds.to_parquet(save_path)  \n    print(ret_preds.sample().T)\n    print(\"--\" * 40)\n\n    # prepare candidate set ---\n    margin = cfg.margin\n    ret_preds[\"threshold\"] = ret_preds[\"pred_scores\"].apply(lambda x: x[0] - margin)\n    ret_preds[\"cutoff\"] = ret_preds.apply(lambda x: sum([y > x[\"threshold\"] for y in x[\"pred_scores\"]]), axis=1)\n    ret_preds[\"cutoff\"] = ret_preds[\"cutoff\"].clip(lower=cfg.min_top_k, upper=cfg.max_top_k)\n\n    print(\"--\" * 40)\n    print(\"Cutoff distribution:\\n-------\")\n    print(ret_preds[\"cutoff\"].value_counts().sort_index())\n    print(f\"Average # candidates: {ret_preds['cutoff'].mean()}\")\n    print(\"--\" * 40)\n\n    ret_preds[\"MisconceptionId\"] = ret_preds.apply(lambda row: row[\"MisconceptionId\"][:row[\"cutoff\"]], axis=1)\n    ret_preds = ret_preds[[\"QuestionId_Answer\", \"MisconceptionId\"]].copy()\n    flat_df = ret_preds[[\"QuestionId_Answer\", \"MisconceptionId\"]].explode([\"MisconceptionId\"]).reset_index(drop=True)\n\n    # prepare ranker input ---\n    rank_df = process_df(test_df)\n    rank_df = rank_df.merge(flat_df, on=\"QuestionId_Answer\", how=\"left\")\n    rank_df = rank_df.merge(concept_df, on=\"MisconceptionId\", how=\"left\")\n    rank_df[\"MisconceptionId\"] = rank_df[\"MisconceptionId\"].astype(str)\n    rank_df = rank_df[\n        [\"QuestionId_Answer\", \"MisconceptionId\", \"SubjectName\", \"ConstructName\", \"QuestionText\", \"CorrectAnswerText\", \"InCorrectAnswerText\", \"AllOptionText\", \"MisconceptionName\"]\n    ].copy()\n    \n    print(\"--\" * 40)\n    save_path = os.path.join(cfg.output_dir, cfg.ranker_input_file_name)\n    print(f\"Saving re-ranker input to: {save_path}\")\n    rank_df.to_parquet(save_path)\n    print(\"--\"*40)\n    \n    print(rank_df.sample().T)\n    print(f\"shape of ranker input: {rank_df.shape}\")\n    print(\"--\"*40)","metadata":{"execution":{"iopub.status.busy":"2024-12-07T21:23:03.609807Z","iopub.execute_input":"2024-12-07T21:23:03.610119Z","iopub.status.idle":"2024-12-07T21:23:03.618394Z","shell.execute_reply.started":"2024-12-07T21:23:03.610089Z","shell.execute_reply":"2024-12-07T21:23:03.617493Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile ranker_prep.yaml\n\nretriever_outputs:\n    - ./retriever_outputs/intfloat.parquet\n    - ./retriever_outputs/qwen_14b.parquet\n\nmin_top_k: 32\nmax_top_k: 64 #64\nmargin: 0.05 # ave n -> 38.2, recall -> 96.5\n\noutput_dir: ./retriever_outputs\nblned_file_name: stage_one_blended.parquet\nranker_input_file_name: ranker_input_stage_one.parquet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:23:03.620754Z","iopub.execute_input":"2024-12-07T21:23:03.621256Z","iopub.status.idle":"2024-12-07T21:23:03.630068Z","shell.execute_reply.started":"2024-12-07T21:23:03.621228Z","shell.execute_reply":"2024-12-07T21:23:03.62919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python prepare_for_ranker.py --config-path ranker_prep.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:23:03.631228Z","iopub.execute_input":"2024-12-07T21:23:03.631972Z","iopub.status.idle":"2024-12-07T21:23:05.578782Z","shell.execute_reply.started":"2024-12-07T21:23:03.631944Z","shell.execute_reply":"2024-12-07T21:23:05.577805Z"},"scrolled":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# COT","metadata":{}},{"cell_type":"code","source":"%%time\n!pip uninstall -y torch\n\n!pip install -q --no-index --find-links=/kaggle/input/wheels-vllm-0-6-3-post1 torchvision==0.19.1\n!pip install -q --no-index --find-links=/kaggle/input/wheels-vllm-0-6-3-post1 vllm\n\n!pip install -q -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -q -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n\n!pip uninstall -y pynvml\n!pip install --no-deps --no-index /kaggle/input/0-6-3-post1-wheels-vllm/nvidia_ml_py-12.560.30-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:23:05.580159Z","iopub.execute_input":"2024-12-07T21:23:05.580497Z","iopub.status.idle":"2024-12-07T21:26:34.908622Z","shell.execute_reply.started":"2024-12-07T21:23:05.580465Z","shell.execute_reply":"2024-12-07T21:26:34.907421Z"},"scrolled":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CoT Script","metadata":{}},{"cell_type":"code","source":"%%writefile run_gen_cot.py\n\nimport argparse\nimport os\nfrom copy import deepcopy\n\nimport pandas as pd\nimport vllm\nprint('vllm version=', vllm.__version__)\n\nfrom datasets import Dataset\nfrom omegaconf import OmegaConf\nfrom transformers import AutoTokenizer\n\nsp = \"Analyze the incorrect answer to detect flaws in the student's reasoning.\"\n\n\ndef get_tokenizer(backbone_path):\n    tokenizer = AutoTokenizer.from_pretrained(backbone_path, add_eos_token=True)\n\n    if tokenizer.eos_token == \"\":\n        tokenizer.add_special_tokens({\"eos_token\": \"</s>\"})\n        tokenizer.eos_token = \"</s>\"\n\n    if tokenizer.pad_token is None:\n        if tokenizer.unk_token is not None:\n            tokenizer.pad_token = tokenizer.unk_token\n            tokenizer.pad_token_id = tokenizer.unk_token_id\n        else:\n            tokenizer.pad_token = tokenizer.eos_token\n            tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    tokenizer.bos_token = \"<|im_start|>\"\n    tokenizer.padding_side = \"left\"\n    return tokenizer\n\n\ndef is_nan(x): return x != x\n\n\ndef eedi_process_df(df):\n    df = deepcopy(df)\n    grouped = df.groupby(\"query_id\")\n\n    question_dict = {}\n    for question_id, group in grouped:\n        question_data = group.to_dict(orient=\"records\")[0]\n        del question_data[\"query_id\"]\n        question_dict[question_id] = question_data\n\n    all_questions = list(question_dict.keys())\n\n    queries = []\n    for qid in all_questions:\n        info = question_dict[qid]\n\n        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n            if info[\"CorrectAnswer\"] == answer_key:\n                continue\n\n            this_example = dict()\n            this_key = f\"{qid}_{answer_key}\"\n            this_example[\"query_id\"] = this_key\n\n            for col in [\"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n                this_example[col] = info[col]\n\n            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n            queries.append(this_example)\n    # --\n    query_df = pd.DataFrame(queries)\n    return query_df\n\n\ndef main(cfg, save_dir, model_id):\n    # load data ---\n    test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n\n    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n        n_ex = int(os.getenv(\"N_EX\"))\n        test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").head(n_ex)\n\n    test_df = test_df.rename(columns={\"QuestionId\": \"query_id\"})\n    test_df = eedi_process_df(test_df)\n\n    content_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n    id2name = dict(zip(content_df['MisconceptionId'], content_df['MisconceptionName']))\n\n\n    ds = Dataset.from_pandas(test_df)\n    print(f\"Number of examples: {len(ds)}\")\n    query_ids = ds[\"query_id\"]\n\n    print(\"==\" * 50)\n    print(f\"Generating for model: {cfg.model.backbone_path}\")\n    print(\"==\" * 50)\n\n    llm = vllm.LLM(\n        cfg.model.backbone_path,\n        tensor_parallel_size=2,\n        quantization=cfg.model.quantization,\n        gpu_memory_utilization=0.99,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=1296,\n        enable_prefix_caching=True\n    )\n\n    tokenizer = get_tokenizer(cfg.model.backbone_path)\n\n    prompts = []\n    for example in ds:\n        question = example[\"QuestionText\"]\n        correct_answer = example[\"CorrectAnswerText\"]\n        incorrect_answer = example[\"InCorrectAnswerText\"]\n\n        user_message = f\"Question: {question}\\nCorrect Answer: {correct_answer}\\nIncorrect Answer: {incorrect_answer}\"\n        text = f\"{sp}\\n\\nQuery: {user_message}\\nAnswer:\\n\"\n        prompts.append(text)\n\n    for p in prompts[:5]:\n        print(p)\n        print(\"-\" * 100)\n\n    sampling_params = vllm.SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.0, max_tokens=cfg.max_new_tokens)\n\n    # do in chunks ---\n    response_dfs = []\n\n    chunk_size = 256\n    for i in range(0, len(prompts), chunk_size):\n        results = []\n        generated_texts = []\n        all_prompts = []\n\n        chunk_prompts = prompts[i : i + chunk_size]\n        chunk_query_ids = query_ids[i : i + chunk_size]\n\n        print(f\"Processing chunk {i//chunk_size + 1} of {(len(prompts)-1)//chunk_size + 1}\")\n\n        generations = llm.generate(chunk_prompts, sampling_params=sampling_params)\n\n        for output in generations:\n            prompt = output.prompt\n            generated_text = output.outputs[0].text\n            all_prompts.append(prompt)\n\n            full_text = f\"{prompt}{generated_text}\"\n            results.append(full_text)\n            generated_texts.append(generated_text)\n\n        # Save intermediate results\n        df = pd.DataFrame()\n        df[\"query_id\"] = chunk_query_ids\n        df[\"prompt\"] = all_prompts\n        df[\"cot\"] = generated_texts\n        response_dfs.append(df)\n\n    result_df = pd.concat(response_dfs).reset_index(drop=True)\n\n    save_path = os.path.join(save_dir, f\"gen_{model_id}.parquet\")\n    result_df.to_parquet(save_path)\n    # ---\n    n = min(5, len(result_df))\n    samples = result_df.sample(n)['cot'].values.tolist()\n    for samp in samples:\n        print(samp)\n        print('--'*50)\n\n\nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--config_path\", type=str, required=True)\n    ap.add_argument(\"--save_dir\", type=str, required=True)\n    ap.add_argument(\"--model_id\", type=str, required=True)\n\n    args = ap.parse_args()\n    cfg = OmegaConf.load(args.config_path)\n\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    # execution ---\n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        main(cfg, save_dir=args.save_dir, model_id=args.model_id)\n    elif cfg.run_on_save:\n        main(cfg, save_dir=args.save_dir, model_id=args.model_id)    \n    else:\n        result_df = pd.read_parquet(\"./gen/gen_qwen_7b.parquet\")\n        save_path = os.path.join(args.save_dir, f\"gen_{args.model_id}.parquet\")\n        result_df.to_parquet(save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:26:34.922103Z","iopub.execute_input":"2024-12-07T21:26:34.92248Z","iopub.status.idle":"2024-12-07T21:26:34.93661Z","shell.execute_reply.started":"2024-12-07T21:26:34.922417Z","shell.execute_reply":"2024-12-07T21:26:34.93575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile conf_llm_gen_7b.yaml\nrun_on_save: true\n\nmax_new_tokens: 256 # 384\nstage_one_path: retriever_outputs/stage_one_blended.parquet\n\nmodel:\n    backbone_path: \"/kaggle/input/eedi-cot-7b-base-dec4/transformers/default/1\"\n    max_length: 768\n    num_proc: 2\n    quantization:\n    \n    tokenizer:\n        padding_side: left\n        truncation_side: left\n        use_fast: true","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:27:49.949921Z","iopub.execute_input":"2024-12-07T21:27:49.950241Z","iopub.status.idle":"2024-12-07T21:27:49.956617Z","shell.execute_reply.started":"2024-12-07T21:27:49.950212Z","shell.execute_reply":"2024-12-07T21:27:49.955582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile conf_llm_gen_14b.yaml\nrun_on_save: false\n\nmax_new_tokens: 256 # 384\nstage_one_path: retriever_outputs/stage_one_blended.parquet\n\nmodel:\n    backbone_path: \"/kaggle/input/eedi-cot-14b-dec6-awq/transformers/default/1\"\n    max_length: 768\n    num_proc: 2\n    quantization: awq\n     \n    tokenizer:\n        padding_side: left\n        truncation_side: left\n        use_fast: true","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:27:50.555063Z","iopub.execute_input":"2024-12-07T21:27:50.555375Z","iopub.status.idle":"2024-12-07T21:27:50.561157Z","shell.execute_reply.started":"2024-12-07T21:27:50.555347Z","shell.execute_reply":"2024-12-07T21:27:50.560236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile conf_llm_gen_32b.yaml\nrun_on_save: false\n\nmax_new_tokens: 256\nstage_one_path: retriever_outputs/stage_one_blended.parquet\n\nmodel:\n    backbone_path: \"/kaggle/input/eedi-cot-32b-dec6-awq/transformers/default/1\"\n    max_length: 768\n    num_proc: 2\n    quantization: awq\n    \n    tokenizer:\n        padding_side: left\n        truncation_side: left\n        use_fast: true","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:27:50.998251Z","iopub.execute_input":"2024-12-07T21:27:50.999108Z","iopub.status.idle":"2024-12-07T21:27:51.004292Z","shell.execute_reply.started":"2024-12-07T21:27:50.999068Z","shell.execute_reply":"2024-12-07T21:27:51.003488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n!python run_gen_cot.py --config_path conf_llm_gen_7b.yaml --save_dir ./gen --model_id qwen_7b","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:27:51.97124Z","iopub.execute_input":"2024-12-07T21:27:51.971991Z","iopub.status.idle":"2024-12-07T21:30:31.1244Z","shell.execute_reply.started":"2024-12-07T21:27:51.971954Z","shell.execute_reply":"2024-12-07T21:30:31.123324Z"},"_kg_hide-output":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n!python run_gen_cot.py --config_path conf_llm_gen_14b.yaml --save_dir ./gen --model_id qwen_14b","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:30:31.126343Z","iopub.execute_input":"2024-12-07T21:30:31.126658Z","iopub.status.idle":"2024-12-07T21:30:43.078319Z","shell.execute_reply.started":"2024-12-07T21:30:31.126627Z","shell.execute_reply":"2024-12-07T21:30:43.077284Z"},"_kg_hide-output":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n!python run_gen_cot.py --config_path conf_llm_gen_32b.yaml --save_dir ./gen --model_id qwen_32b","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:30:43.079619Z","iopub.execute_input":"2024-12-07T21:30:43.079889Z","iopub.status.idle":"2024-12-07T21:30:53.698788Z","shell.execute_reply.started":"2024-12-07T21:30:43.079862Z","shell.execute_reply":"2024-12-07T21:30:53.69771Z"},"_kg_hide-output":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for 256 inputs\n# 7b gen time  : ~2 mins\n# 14b gen time : \n# 32b gen time : ~12mins","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:30:53.701921Z","iopub.execute_input":"2024-12-07T21:30:53.702267Z","iopub.status.idle":"2024-12-07T21:30:53.706408Z","shell.execute_reply.started":"2024-12-07T21:30:53.702236Z","shell.execute_reply":"2024-12-07T21:30:53.705506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 2: Re-Ranker (Haiku)","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Scripts","metadata":{}},{"cell_type":"code","source":"%%writefile run_haiku.py\n\nimport sys\nsys.path.insert(0, \"/kaggle/input/eedi-utils-v11\")\n\nimport argparse\nimport gc\nimport os\nimport vllm\n\nimport random\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom llm_oracle.ranker_dataset import RankerDataset\nfrom omegaconf import OmegaConf\n\n\nfrom copy import deepcopy\n\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\n\ndef is_nan(x): return x != x\n    \ndef stable_softmax(x, temp=1.0):\n    x = np.array(x) / temp\n    x_max = np.max(x)\n    exp_x = np.exp(x - x_max)\n    return exp_x / np.sum(exp_x)\n    \ndef eedi_process_df(df):\n    df = df.copy()\n    df = df.rename(columns={\"QuestionId\": \"query_id\"})\n    grouped = df.groupby(\"query_id\")\n\n    question_dict = {}\n    for question_id, group in grouped:\n        question_data = group.to_dict(orient=\"records\")[0]\n        del question_data[\"query_id\"]\n        question_dict[question_id] = question_data\n\n    all_questions = list(question_dict.keys())\n\n    queries = []\n    query2content = defaultdict(list)\n    content2query = defaultdict(list)\n\n    # ---\n    for qid in all_questions:\n        info = question_dict[qid]\n\n        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n            if info[\"CorrectAnswer\"] == answer_key:\n                continue\n            this_example = dict()\n            this_key = f\"{qid}_{answer_key}\"\n            this_example[\"query_id\"] = this_key\n\n            if is_nan(info[f\"Misconception{answer_key}Id\"]):\n                continue\n\n            mid = str(int(info[f\"Misconception{answer_key}Id\"]))\n            query2content[this_key].append(mid)\n            content2query[mid].append(this_key)\n\n            # ---\n            for col in [\"SubjectId\", \"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n                this_example[col] = info[col]\n\n            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n            this_example[\"AllOptionText\"] = \"\\n- \".join([info[f\"Answer{x}Text\"] for x in [\"A\", \"B\", \"C\", \"D\"]])\n            this_example[\"AllOptionText\"] = f\"\\n- {this_example['AllOptionText']}\"\n            queries.append(this_example)\n    # --\n    query_df = pd.DataFrame(queries)\n    corr_df = pd.Series(query2content).reset_index().rename(columns={\"index\": \"query_id\", 0: \"content_id\"})\n    corr_df[\"content_id\"] = corr_df[\"content_id\"].apply(lambda x: x[0])\n\n    query_df = query_df.reset_index(drop=True)\n\n    return query_df, corr_df, content2query\n\ndef sort_by_scores(pred_ids, scores):\n    keep_idxs = np.argsort(-np.array(scores)).tolist()\n    ret_ids = [pred_ids[idx] for idx in keep_idxs]\n    ret_scores = [scores[idx] for idx in keep_idxs]\n    return {\"sorted_ids\": ret_ids, \"sorted_scores\": ret_scores}\n\n\ndef format_example(row):\n    example = f\"Question: {row['QuestionText']}\\nAnswer:{row['CorrectAnswerText']}\\nMisconception Answer: {row['InCorrectAnswerText']}\"\n    return example\n\n\ndef add_fs_examples(df, content2query, query2example, rng, n=2):\n    cache = {}\n    def _add_examples(row):\n        cid = row[\"content_id\"]\n        if cid in cache:\n            return cache[cid]\n        else:\n            qids = content2query[cid]\n            qids = [qid for qid in qids if qid != row[\"query_id\"]]\n            if len(qids) == 0:\n                cache[cid] = \"\"\n                return \"\"\n\n            qids = rng.sample(qids, k=min(n, len(qids)))\n            examples = [query2example[qid] for qid in qids]\n            fs = \"\\n--\\n\".join(examples)\n            cache[cid] = fs\n            return fs\n\n    df[\"examples\"] = df.apply(_add_examples, axis=1)\n    return df\n\n\ndef main(cfg, save_dir, model_id):\n    test_df = pd.read_parquet(cfg.input_path)\n    test_df = test_df.rename(columns={\"QuestionId_Answer\": \"query_id\", \"MisconceptionId\": \"content_id\"})\n    test_df[\"content_id\"] = test_df[\"content_id\"].astype(str)\n\n    # comp data examples ---\n    rng = random.Random(cfg.seed)\n    comp_df = pd.read_csv(cfg.icl_path).rename(columns={\"QuestionId\": \"query_id\"})\n\n    query_df, _, content2query = eedi_process_df(comp_df)\n    query_df[\"demo\"] = query_df.apply(format_example, axis=1)\n    query2example = dict(zip(query_df[\"query_id\"], query_df[\"demo\"]))\n\n    # add few shot examples --\n    test_df = add_fs_examples(test_df, content2query, query2example, rng, n=cfg.k_shot)\n    test_df = test_df.sort_values(by='content_id').reset_index(drop=True)\n\n    # cot --\n    if cfg.use_cot:\n        print(\"Loading CoT....\")\n        cot_df = pd.read_parquet(cfg.cot_path)\n        test_df = test_df.merge(cot_df, on='query_id', how='left')\n        num_missing = test_df[\"cot\"].isna().sum()\n        print(f\"# of missing cot: {num_missing}\")\n        test_df[\"cot\"] = test_df[\"cot\"].fillna(\"\")\n        print(test_df.sample().T.to_dict())\n        print(\"--\"*40)\n\n    #---\n    dataset_creator = RankerDataset(cfg)\n    infer_ds = dataset_creator.get_dataset(test_df)\n    \n    tokenizer = dataset_creator.tokenizer    \n    infer_qa_ids = infer_ds[\"query_id\"]\n    infer_mc_ids = infer_ds[\"content_id\"]\n    \n    infer_ds = infer_ds.map(lambda example: {\"prompt\": tokenizer.decode(example['input_ids'], skip_special_tokens=False)})\n    prompts = infer_ds['prompt']\n\n    print(f\"# of requests: {len(prompts)}\")\n    print(f\"Example:\\n\\n{prompts[0]}\")\n    \n    # -- in\n    llm = vllm.LLM(\n        cfg.model.backbone_path,\n        quantization=\"awq\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.99,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2048,\n        enable_prefix_caching=True,\n    )\n    \n    sampling_params = vllm.SamplingParams(n=1, top_p=0.8, logprobs=20, max_tokens=1, temperature=0.0, skip_special_tokens=False)\n    responses = llm.generate(prompts, sampling_params, use_tqdm=True)\n\n    # Get Results\n    print(\"--\"*40)\n    yes_tok_id = tokenizer(\"Yes\", add_special_tokens=False)[\"input_ids\"][-1]\n    no_tok_id = tokenizer(\"No\", add_special_tokens=False)[\"input_ids\"][-1]\n    \n    print(f\">> EediRanker: Yes token id: {yes_tok_id} | Expected: 9454\")\n    print(f\">> EediRanker: No token id: {no_tok_id} | Expected: 2753\")\n    print(\"--\"*40)\n    \n    QuestionId_Answer = []\n    MisconceptionId =[]\n    scores = []\n    \n    for qid, cid, response in zip(infer_qa_ids, infer_mc_ids, responses):\n        logprob_dict = response.outputs[0].logprobs[0]\n\n        top_tok_ids = set(list(logprob_dict.keys()))\n        if len(top_tok_ids.intersection(set([yes_tok_id, no_tok_id]))) == 0:\n            print(f\"Bad Output for {qid} - {cid}\")\n            continue\n        \n        yes_logit, no_logit = -10.0, -10.0\n        \n        if yes_tok_id in logprob_dict:\n            yes_logit = logprob_dict[yes_tok_id].logprob\n        \n        if no_tok_id in logprob_dict:\n            no_logit = logprob_dict[no_tok_id].logprob\n        \n        score = yes_logit - no_logit\n        \n        QuestionId_Answer.append(qid)\n        MisconceptionId.append(cid)\n        scores.append(score)\n    \n    result_df = pd.DataFrame()\n    result_df[\"QuestionId_Answer\"] = QuestionId_Answer\n    result_df[\"MisconceptionId\"] = MisconceptionId\n    result_df[\"score\"] = scores\n    \n    agg_df = result_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n    score_agg_df = result_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n    agg_df = pd.merge(agg_df, score_agg_df, on=\"QuestionId_Answer\", how=\"left\")\n    \n    agg_df[\"topk_info\"] = agg_df.apply(lambda x: sort_by_scores(x[\"MisconceptionId\"], x[\"score\"]), axis=1)\n    agg_df[\"MisconceptionId\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_ids\"])\n    agg_df[\"score\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_scores\"])\n    \n    # compute oof dataframe ---\n    oof_df = agg_df.copy()\n    oof_df = oof_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n    oof_df = oof_df.rename(columns={\"score\": \"logit_scores\"})\n\n    # normalize ---\n    oof_df[\"pred_scores\"] = oof_df[\"logit_scores\"].apply(stable_softmax)\n    oof_df[\"MisconceptionId\"] = oof_df[\"MisconceptionId\"].apply(lambda x: list(map(str, x)))\n\n    # print ---\n    print(\"--\"*40)\n    row = oof_df.sample()\n    formatted_scores = [f\"{s:.3f}\" for s in row['pred_scores'].values[0]]\n    misconceptions = row['MisconceptionId'].values[0]\n    print(f\"Showing 1 example: {row['QuestionId_Answer'].values[0]}\")\n    for rank, (m, s) in enumerate(zip(misconceptions, formatted_scores)):\n        print(f\"MisconceptionId: {m} -> Score: {s}\")\n    print(\"--\"*40)\n\n    save_path = os.path.join(save_dir, f\"ranker_{model_id}.parquet\")\n    oof_df.to_parquet(save_path)\n\nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument('--config_path', type=str, required=True)\n    ap.add_argument('--save_dir', type=str, required=True)\n    ap.add_argument('--model_id', type=str, required=True)\n\n    args = ap.parse_args()\n    cfg = OmegaConf.load(args.config_path)\n\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    # execution ---\n    main(cfg, save_dir=args.save_dir, model_id=args.model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:30:53.707789Z","iopub.execute_input":"2024-12-07T21:30:53.708123Z","iopub.status.idle":"2024-12-07T21:30:53.723448Z","shell.execute_reply.started":"2024-12-07T21:30:53.708053Z","shell.execute_reply":"2024-12-07T21:30:53.722498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2 Re-Ranker Config","metadata":{}},{"cell_type":"code","source":"%%writefile conf_llm_oracle_14b_awq.yaml\n\nseed: 675\ninput_path: ./retriever_outputs/ranker_input_stage_one.parquet\nicl_path: /kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\n\nk_shot: 2\nuse_cot: false\ncot_path: ./gen/gen_qwen_14b.parquet\n\nmodel:\n    backbone_path: \"/kaggle/input/eedi-oracle-14b-dec7-cv646-awq-ff/transformers/default/1\"\n    max_length: 768\n    num_proc: 2\n    \n    tokenizer:\n        padding_side: left\n        truncation_side: left\n        use_fast: true","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:30:53.724592Z","iopub.execute_input":"2024-12-07T21:30:53.725559Z","iopub.status.idle":"2024-12-07T21:30:53.736324Z","shell.execute_reply.started":"2024-12-07T21:30:53.725527Z","shell.execute_reply":"2024-12-07T21:30:53.735597Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3 Re-Ranker Execution","metadata":{}},{"cell_type":"code","source":"%%time\n!python run_haiku.py --config_path \"./conf_llm_oracle_14b_awq.yaml\" --save_dir \"./ranker_outputs\" --model_id \"qwen_14b\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:30:53.73735Z","iopub.execute_input":"2024-12-07T21:30:53.737671Z","iopub.status.idle":"2024-12-07T21:39:17.955038Z","shell.execute_reply.started":"2024-12-07T21:30:53.737635Z","shell.execute_reply":"2024-12-07T21:39:17.953994Z"},"scrolled":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.4 Blend Retriever and Ranker","metadata":{}},{"cell_type":"code","source":"%%writefile blend_one_two.py\n\nimport argparse\nimport os\nfrom copy import deepcopy\n\nimport pandas as pd\nimport numpy as np\nfrom omegaconf import OmegaConf\n\ndef get_sorted_pairs(content_ids, scores):\n    collection = [(cid, s) for cid, s in zip(content_ids, scores)]\n    sorted_collection = sorted(collection, key=lambda x: x[1], reverse=True)\n    return sorted_collection\n\ndef cut_at_n(sub_df, n=25):\n    sub_df[\"MisconceptionId\"] = sub_df[\"MisconceptionId\"].apply(lambda x: x[:n])\n    sub_df[\"score\"] = sub_df[\"score\"].apply(lambda x: x[:n])\n    return sub_df\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config-path\", type=str)\n    args = parser.parse_args()\n    \n    with open(args.config_path, \"r\") as f:\n        cfg = OmegaConf.load(f)\n\n    # read predictions ---\n    ret_preds = pd.read_parquet(cfg.ret_path)\n    ranker_preds = pd.read_parquet(cfg.ranker_path)\n    print(ranker_preds.sample(3))\n\n    # flatten ---\n    ret_preds = ret_preds[[\"QuestionId_Answer\", \"MisconceptionId\", \"pred_scores\"]].explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n    ret_preds = ret_preds.rename(columns={\"pred_scores\": \"score_ret\"})\n    ret_preds[\"MisconceptionId\"] = ret_preds[\"MisconceptionId\"].astype(str)\n\n    ranker_preds = ranker_preds[[\"QuestionId_Answer\", \"MisconceptionId\", \"pred_scores\"]].explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n    ranker_preds = ranker_preds.rename(columns={\"pred_scores\": \"score_ranker\"})\n    ranker_preds[\"MisconceptionId\"] = ranker_preds[\"MisconceptionId\"].astype(str)\n\n    # blend ---\n    w_ret = cfg.ret_weight\n    w_ranker = cfg.ranker_weight\n\n    candidate_df = pd.merge(ret_preds, ranker_preds, on=[\"QuestionId_Answer\", \"MisconceptionId\"])\n    candidate_df[\"score\"] = candidate_df.apply(lambda x: w_ret*x['score_ret'] + w_ranker*x['score_ranker'], axis=1) # blending\n    candidate_df = candidate_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n\n    cdf = candidate_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n    sdf = candidate_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n    candidate_df = pd.merge(cdf, sdf, on=\"QuestionId_Answer\", how=\"left\")\n\n    candidate_df[\"sorted\"] = candidate_df.apply(lambda x: get_sorted_pairs(x['MisconceptionId'], x['score']), axis=1)\n    candidate_df[\"MisconceptionId\"] = candidate_df[\"sorted\"].apply(lambda x: [y[0] for y in x])\n    candidate_df[\"score\"] = candidate_df[\"sorted\"].apply(lambda x: [y[1] for y in x])\n    candidate_df = candidate_df.drop(columns=['sorted'])\n    \n    print(\"--\"*40)\n    print(f\"saving retriever+ranker prediction to: {cfg.blended_pred_path}\")\n    candidate_df.to_parquet(cfg.blended_pred_path)\n    print(\"Example:\")\n    print(candidate_df.sample().T)\n    print(\"--\"*40)\n\n    # Cut at N ---\n    candidate_df = cut_at_n(candidate_df, n=cfg.cutoff_n)\n    input_df = pd.read_parquet(cfg.ranker_input_file)\n    print(f\"Shape of ranker input previously: {input_df.shape}\")\n    keep_df = candidate_df[['QuestionId_Answer', 'MisconceptionId']].explode(\"MisconceptionId\").reset_index(drop=True)\n    keep_df['MisconceptionId'] = keep_df['MisconceptionId'].astype(input_df[\"MisconceptionId\"].dtype)\n    input_df = input_df.merge(keep_df, on=[\"QuestionId_Answer\", \"MisconceptionId\"], how=\"inner\")\n    print(f\"shape of ranker input for next stage: {input_df.shape}\")\n\n    # # Prepare further ranking ---\n    save_path = cfg.reranker_input_path\n    print(f\"saving output to: {save_path}\")\n    input_df.to_parquet(save_path)\n    print(f\"shape of input_df output: {input_df.shape}\")\n    print(\"--\"*40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:39:21.953262Z","iopub.execute_input":"2024-12-07T21:39:21.954078Z","iopub.status.idle":"2024-12-07T21:39:21.961652Z","shell.execute_reply.started":"2024-12-07T21:39:21.954042Z","shell.execute_reply":"2024-12-07T21:39:21.96063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile one_two_blend.yaml\n\nranker_input_file: ./retriever_outputs/ranker_input_stage_one.parquet\n\nret_path: ./retriever_outputs/stage_one_blended.parquet\nranker_path: ./ranker_outputs/ranker_qwen_14b.parquet\n\nret_weight: 0.0\nranker_weight: 1.0\n\ncutoff_n: 8 # recall ~0.82\n\nblended_pred_path: ./ranker_outputs/one_two_blended.parquet\nreranker_input_path: ./ranker_outputs/ranker_input_stage_two.parquet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:39:23.810781Z","iopub.execute_input":"2024-12-07T21:39:23.811504Z","iopub.status.idle":"2024-12-07T21:39:23.816998Z","shell.execute_reply.started":"2024-12-07T21:39:23.811458Z","shell.execute_reply":"2024-12-07T21:39:23.815978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python blend_one_two.py --config-path one_two_blend.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:39:26.465207Z","iopub.execute_input":"2024-12-07T21:39:26.466171Z","iopub.status.idle":"2024-12-07T21:39:28.351841Z","shell.execute_reply.started":"2024-12-07T21:39:26.466135Z","shell.execute_reply":"2024-12-07T21:39:28.350788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optional Sub","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n\n# pred_df = pd.read_parquet(\"./ranker_outputs/one_two_blended.parquet\")\n# pred_df[\"MisconceptionId\"] = pred_df[\"MisconceptionId\"].apply(lambda x: x[:25])\n# pred_df[\"MisconceptionId\"] = pred_df[\"MisconceptionId\"].apply(lambda x: \" \".join(x))\n# sub_df = pred_df[[\"QuestionId_Answer\", \"MisconceptionId\"]].copy()\n# sub_df.to_csv(\"submission.csv\", index=False)\n\n# sub_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:39:33.368241Z","iopub.execute_input":"2024-12-07T21:39:33.368629Z","iopub.status.idle":"2024-12-07T21:39:33.373598Z","shell.execute_reply.started":"2024-12-07T21:39:33.368592Z","shell.execute_reply":"2024-12-07T21:39:33.372469Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 3: Reranker (Sonnet)","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Scripts","metadata":{}},{"cell_type":"code","source":"%%writefile run_sonnet_v1.py\n\nimport sys\nsys.path.insert(0, \"/kaggle/input/eedi-utils-v08\")\n\nimport argparse\nimport gc\nimport os\nimport vllm\n\nimport random\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom llm_oracle.ranker_dataset import RankerDataset\nfrom llm_oracle.ranker_loader import RankerCollator, RankerCollatorTrain, show_batch\nfrom llm_oracle.ranker_model import EediRanker\n\nfrom omegaconf import OmegaConf\n\ndef is_nan(x): return x != x\n    \ndef stable_softmax(x, temp=1.0):\n    x = np.array(x) / temp\n    x_max = np.max(x)\n    exp_x = np.exp(x - x_max)\n    return exp_x / np.sum(exp_x)\n    \ndef eedi_process_df(df):\n    df = df.copy()\n    df = df.rename(columns={\"QuestionId\": \"query_id\"})\n    grouped = df.groupby(\"query_id\")\n\n    question_dict = {}\n    for question_id, group in grouped:\n        question_data = group.to_dict(orient=\"records\")[0]\n        del question_data[\"query_id\"]\n        question_dict[question_id] = question_data\n\n    all_questions = list(question_dict.keys())\n\n    queries = []\n    query2content = defaultdict(list)\n    content2query = defaultdict(list)\n\n    # ---\n    for qid in all_questions:\n        info = question_dict[qid]\n\n        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n            if info[\"CorrectAnswer\"] == answer_key:\n                continue\n            this_example = dict()\n            this_key = f\"{qid}_{answer_key}\"\n            this_example[\"query_id\"] = this_key\n\n            if is_nan(info[f\"Misconception{answer_key}Id\"]):\n                continue\n\n            mid = str(int(info[f\"Misconception{answer_key}Id\"]))\n            query2content[this_key].append(mid)\n            content2query[mid].append(this_key)\n\n            # ---\n            for col in [\"SubjectId\", \"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n                this_example[col] = info[col]\n\n            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n            this_example[\"AllOptionText\"] = \"\\n- \".join([info[f\"Answer{x}Text\"] for x in [\"A\", \"B\", \"C\", \"D\"]])\n            this_example[\"AllOptionText\"] = f\"\\n- {this_example['AllOptionText']}\"\n            queries.append(this_example)\n    # --\n    query_df = pd.DataFrame(queries)\n    corr_df = pd.Series(query2content).reset_index().rename(columns={\"index\": \"query_id\", 0: \"content_id\"})\n    corr_df[\"content_id\"] = corr_df[\"content_id\"].apply(lambda x: x[0])\n\n    query_df = query_df.reset_index(drop=True)\n\n    return query_df, corr_df, content2query\n\ndef sort_by_scores(pred_ids, scores):\n    keep_idxs = np.argsort(-np.array(scores)).tolist()\n    ret_ids = [pred_ids[idx] for idx in keep_idxs]\n    ret_scores = [scores[idx] for idx in keep_idxs]\n    return {\"sorted_ids\": ret_ids, \"sorted_scores\": ret_scores}\n\n\ndef format_example(row):\n    example = f\"Question: {row['QuestionText']}\\nAnswer:{row['CorrectAnswerText']}\\nMisconception Answer: {row['InCorrectAnswerText']}\"\n    return example\n\ndef add_fs_examples(df, content2query, query2example, rng, n=2):\n    cache = {}\n    def _add_examples(row):\n        cid = row[\"content_id\"]\n        if cid in cache:\n            return cache[cid]\n        else:\n            qids = content2query[cid]\n            qids = [qid for qid in qids if qid != row[\"query_id\"]]\n            if len(qids) == 0:\n                cache[cid] = \"\"\n                return \"\"\n\n            qids = rng.sample(qids, k=min(n, len(qids)))\n            examples = [query2example[qid] for qid in qids]\n            fs = \"\\n--\\n\".join(examples)\n            cache[cid] = fs\n            return fs\n\n    df[\"examples\"] = df.apply(_add_examples, axis=1)\n    return df\n\n\ndef main(cfg, save_dir, model_id):\n    test_df = pd.read_parquet(cfg.input_path)\n    test_df = test_df.rename(columns={\"QuestionId_Answer\": \"query_id\", \"MisconceptionId\": \"content_id\"})\n    test_df[\"content_id\"] = test_df[\"content_id\"].astype(str)\n\n    # comp data examples ---\n    rng = random.Random(cfg.seed)\n    comp_df = pd.read_csv(cfg.icl_path).rename(columns={\"QuestionId\": \"query_id\"})\n\n\n    query_df, _, content2query = eedi_process_df(comp_df)\n    query_df[\"demo\"] = query_df.apply(format_example, axis=1)\n    query2example = dict(zip(query_df[\"query_id\"], query_df[\"demo\"]))\n\n    # add few shot examples --\n    test_df = add_fs_examples(test_df, content2query, query2example, rng, n=cfg.k_shot)\n    test_df = test_df.sort_values(by='content_id').reset_index(drop=True)\n\n    # cot --\n    if cfg.use_cot:\n        print(\"Loading CoT....\")\n        cot_df = pd.read_parquet(cfg.cot_path)\n        test_df = test_df.merge(cot_df, on='query_id', how='left')\n        num_missing = test_df[\"cot\"].isna().sum()\n        print(f\"# of missing cot: {num_missing}\")\n        test_df[\"cot\"] = test_df[\"cot\"].fillna(\"\")\n        print(test_df.sample().T.to_dict())\n        print(\"--\"*40)\n\n    #---\n    dataset_creator = RankerDataset(cfg)\n    infer_ds = dataset_creator.get_dataset(test_df)\n    \n    tokenizer = dataset_creator.tokenizer\n    infer_ds = infer_ds.map(lambda example: {\"prompt\": tokenizer.decode(example['input_ids'], skip_special_tokens=False)})\n    \n    infer_qa_ids = infer_ds[\"query_id\"]\n    infer_mc_ids = infer_ds[\"content_id\"]\n    \n    prompts = infer_ds['prompt']\n\n    print(f\"# of requests: {len(prompts)}\")\n    print(f\"Example:\\n\\n{prompts[0]}\")\n    \n    # -- in\n    llm = vllm.LLM(\n        cfg.model.backbone_path,\n        quantization=\"awq\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.99,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2048,\n        enable_prefix_caching=True\n    )\n    \n    sampling_params = vllm.SamplingParams(n=1, top_p=0.8, logprobs=20, max_tokens=1, temperature=0.0, skip_special_tokens=False)\n    responses = llm.generate(prompts, sampling_params, use_tqdm=True)\n\n    # Get Results\n    print(\"--\"*40)\n    yes_tok_id = tokenizer(\"Yes\", add_special_tokens=False)[\"input_ids\"][-1]\n    no_tok_id = tokenizer(\"No\", add_special_tokens=False)[\"input_ids\"][-1]\n    \n    print(f\">> EediRanker: Yes token id: {yes_tok_id} | Expected: 9454\")\n    print(f\">> EediRanker: No token id: {no_tok_id} | Expected: 2753\")\n    print(\"--\"*40)\n    \n    QuestionId_Answer = []\n    MisconceptionId =[]\n    scores = []\n    \n    for qid, cid, response in zip(infer_qa_ids, infer_mc_ids, responses):\n        logprob_dict = response.outputs[0].logprobs[0]\n\n        top_tok_ids = set(list(logprob_dict.keys()))\n        if len(top_tok_ids.intersection(set([yes_tok_id, no_tok_id]))) == 0:\n            print(f\"Bad Output for {qid} - {cid}\")\n            continue\n        \n        yes_logit, no_logit = -10.0, -10.0\n        \n        if yes_tok_id in logprob_dict:\n            yes_logit = logprob_dict[yes_tok_id].logprob\n        \n        if no_tok_id in logprob_dict:\n            no_logit = logprob_dict[no_tok_id].logprob\n        \n        score = yes_logit - no_logit\n        \n        QuestionId_Answer.append(qid)\n        MisconceptionId.append(cid)\n        scores.append(score)\n    \n    result_df = pd.DataFrame()\n    result_df[\"QuestionId_Answer\"] = QuestionId_Answer\n    result_df[\"MisconceptionId\"] = MisconceptionId\n    result_df[\"score\"] = scores\n    \n    agg_df = result_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n    score_agg_df = result_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n    agg_df = pd.merge(agg_df, score_agg_df, on=\"QuestionId_Answer\", how=\"left\")\n    \n    agg_df[\"topk_info\"] = agg_df.apply(lambda x: sort_by_scores(x[\"MisconceptionId\"], x[\"score\"]), axis=1)\n    agg_df[\"MisconceptionId\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_ids\"])\n    agg_df[\"score\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_scores\"])\n    \n    # compute oof dataframe ---\n    oof_df = agg_df.copy()\n    oof_df = oof_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n    oof_df = oof_df.rename(columns={\"score\": \"logit_scores\"})\n\n    # normalize ---\n    oof_df[\"pred_scores\"] = oof_df[\"logit_scores\"].apply(stable_softmax)\n    oof_df[\"MisconceptionId\"] = oof_df[\"MisconceptionId\"].apply(lambda x: list(map(str, x)))\n\n    # print ---\n    print(\"--\"*40)\n    row = oof_df.sample()\n    formatted_scores = [f\"{s:.3f}\" for s in row['pred_scores'].values[0]]\n    misconceptions = row['MisconceptionId'].values[0]\n    print(f\"Showing 1 example: {row['QuestionId_Answer'].values[0]}\")\n    for rank, (m, s) in enumerate(zip(misconceptions, formatted_scores)):\n        print(f\"MisconceptionId: {m} -> Score: {s}\")\n    print(\"--\"*40)\n\n    save_path = os.path.join(save_dir, f\"ranker_{model_id}.parquet\")\n    oof_df.to_parquet(save_path)\n\nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument('--config_path', type=str, required=True)\n    ap.add_argument('--save_dir', type=str, required=True)\n    ap.add_argument('--model_id', type=str, required=True)\n\n    args = ap.parse_args()\n    cfg = OmegaConf.load(args.config_path)\n\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    # execution ---\n    main(cfg, save_dir=args.save_dir, model_id=args.model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:39:36.839153Z","iopub.execute_input":"2024-12-07T21:39:36.839521Z","iopub.status.idle":"2024-12-07T21:39:36.850374Z","shell.execute_reply.started":"2024-12-07T21:39:36.839488Z","shell.execute_reply":"2024-12-07T21:39:36.849478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile run_sonnet_v2.py\n\nimport sys\nsys.path.insert(0, \"/kaggle/input/eedi-utils-v12\")\n\nimport argparse\nimport gc\nimport os\nimport vllm\n\nimport random\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom llm_oracle.ranker_dataset import RankerDataset\nfrom llm_oracle.ranker_loader import RankerCollator, RankerCollatorTrain, show_batch\nfrom llm_oracle.ranker_model import EediRanker\n\nfrom omegaconf import OmegaConf\n\ndef is_nan(x): return x != x\n    \ndef stable_softmax(x, temp=1.0):\n    x = np.array(x) / temp\n    x_max = np.max(x)\n    exp_x = np.exp(x - x_max)\n    return exp_x / np.sum(exp_x)\n    \ndef eedi_process_df(df):\n    df = df.copy()\n    df = df.rename(columns={\"QuestionId\": \"query_id\"})\n    grouped = df.groupby(\"query_id\")\n\n    question_dict = {}\n    for question_id, group in grouped:\n        question_data = group.to_dict(orient=\"records\")[0]\n        del question_data[\"query_id\"]\n        question_dict[question_id] = question_data\n\n    all_questions = list(question_dict.keys())\n\n    queries = []\n    query2content = defaultdict(list)\n    content2query = defaultdict(list)\n\n    # ---\n    for qid in all_questions:\n        info = question_dict[qid]\n\n        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n            if info[\"CorrectAnswer\"] == answer_key:\n                continue\n            this_example = dict()\n            this_key = f\"{qid}_{answer_key}\"\n            this_example[\"query_id\"] = this_key\n\n            if is_nan(info[f\"Misconception{answer_key}Id\"]):\n                continue\n\n            mid = str(int(info[f\"Misconception{answer_key}Id\"]))\n            query2content[this_key].append(mid)\n            content2query[mid].append(this_key)\n\n            # ---\n            for col in [\"SubjectId\", \"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n                this_example[col] = info[col]\n\n            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n            this_example[\"AllOptionText\"] = \"\\n- \".join([info[f\"Answer{x}Text\"] for x in [\"A\", \"B\", \"C\", \"D\"]])\n            this_example[\"AllOptionText\"] = f\"\\n- {this_example['AllOptionText']}\"\n            queries.append(this_example)\n    # --\n    query_df = pd.DataFrame(queries)\n    corr_df = pd.Series(query2content).reset_index().rename(columns={\"index\": \"query_id\", 0: \"content_id\"})\n    corr_df[\"content_id\"] = corr_df[\"content_id\"].apply(lambda x: x[0])\n\n    query_df = query_df.reset_index(drop=True)\n\n    return query_df, corr_df, content2query\n\ndef sort_by_scores(pred_ids, scores):\n    keep_idxs = np.argsort(-np.array(scores)).tolist()\n    ret_ids = [pred_ids[idx] for idx in keep_idxs]\n    ret_scores = [scores[idx] for idx in keep_idxs]\n    return {\"sorted_ids\": ret_ids, \"sorted_scores\": ret_scores}\n\n\ndef format_example(row):\n    example = f\"Question: {row['QuestionText']}\\nAnswer:{row['CorrectAnswerText']}\\nMisconception Answer: {row['InCorrectAnswerText']}\"\n    return example\n\ndef add_fs_examples(df, content2query, query2example, rng, n=2):\n    cache = {}\n    def _add_examples(row):\n        cid = row[\"content_id\"]\n        if cid in cache:\n            return cache[cid]\n        else:\n            qids = content2query[cid]\n            qids = [qid for qid in qids if qid != row[\"query_id\"]]\n            if len(qids) == 0:\n                cache[cid] = \"\"\n                return \"\"\n\n            qids = rng.sample(qids, k=min(n, len(qids)))\n            examples = [query2example[qid] for qid in qids]\n            fs = \"\\n--\\n\".join(examples)\n            cache[cid] = fs\n            return fs\n\n    df[\"examples\"] = df.apply(_add_examples, axis=1)\n    return df\n\n\ndef main(cfg, save_dir, model_id):\n    test_df = pd.read_parquet(cfg.input_path)\n    test_df = test_df.rename(columns={\"QuestionId_Answer\": \"query_id\", \"MisconceptionId\": \"content_id\"})\n    test_df[\"content_id\"] = test_df[\"content_id\"].astype(str)\n\n    # comp data examples ---\n    rng = random.Random(cfg.seed)\n    comp_df = pd.read_csv(cfg.icl_path).rename(columns={\"QuestionId\": \"query_id\"})\n\n\n    query_df, _, content2query = eedi_process_df(comp_df)\n    query_df[\"demo\"] = query_df.apply(format_example, axis=1)\n    query2example = dict(zip(query_df[\"query_id\"], query_df[\"demo\"]))\n\n    # add few shot examples --\n    test_df = add_fs_examples(test_df, content2query, query2example, rng, n=cfg.k_shot)\n    test_df = test_df.sort_values(by='content_id').reset_index(drop=True)\n\n    # cot --\n    if cfg.use_cot:\n        print(\"Loading CoT....\")\n        cot_df = pd.read_parquet(cfg.cot_path)\n        test_df = test_df.merge(cot_df, on='query_id', how='left')\n        num_missing = test_df[\"cot\"].isna().sum()\n        print(f\"# of missing cot: {num_missing}\")\n        test_df[\"cot\"] = test_df[\"cot\"].fillna(\"\")\n        print(test_df.sample().T.to_dict())\n        print(\"--\"*40)\n\n    #---\n    dataset_creator = RankerDataset(cfg)\n    infer_ds = dataset_creator.get_dataset(test_df)\n    \n    tokenizer = dataset_creator.tokenizer\n    infer_ds = infer_ds.map(lambda example: {\"prompt\": tokenizer.decode(example['input_ids'], skip_special_tokens=False)})\n    \n    infer_qa_ids = infer_ds[\"query_id\"]\n    infer_mc_ids = infer_ds[\"content_id\"]\n    \n    prompts = infer_ds['prompt']\n\n    print(f\"# of requests: {len(prompts)}\")\n    print(f\"Example:\\n\\n{prompts[0]}\")\n    \n    # -- in\n    llm = vllm.LLM(\n        cfg.model.backbone_path,\n        quantization=\"awq\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.99,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2048,\n        enable_prefix_caching=True\n    )\n    \n    sampling_params = vllm.SamplingParams(n=1, top_p=0.8, logprobs=20, max_tokens=1, temperature=0.0, skip_special_tokens=False)\n    responses = llm.generate(prompts, sampling_params, use_tqdm=True)\n\n    # Get Results\n    print(\"--\"*40)\n    yes_tok_id = tokenizer(\"Yes\", add_special_tokens=False)[\"input_ids\"][-1]\n    no_tok_id = tokenizer(\"No\", add_special_tokens=False)[\"input_ids\"][-1]\n    \n    print(f\">> EediRanker: Yes token id: {yes_tok_id} | Expected: 9454\")\n    print(f\">> EediRanker: No token id: {no_tok_id} | Expected: 2753\")\n    print(\"--\"*40)\n    \n    QuestionId_Answer = []\n    MisconceptionId =[]\n    scores = []\n    \n    for qid, cid, response in zip(infer_qa_ids, infer_mc_ids, responses):\n        logprob_dict = response.outputs[0].logprobs[0]\n\n        top_tok_ids = set(list(logprob_dict.keys()))\n        if len(top_tok_ids.intersection(set([yes_tok_id, no_tok_id]))) == 0:\n            print(f\"Bad Output for {qid} - {cid}\")\n            continue\n        \n        yes_logit, no_logit = -10.0, -10.0\n        \n        if yes_tok_id in logprob_dict:\n            yes_logit = logprob_dict[yes_tok_id].logprob\n        \n        if no_tok_id in logprob_dict:\n            no_logit = logprob_dict[no_tok_id].logprob\n        \n        score = yes_logit - no_logit\n        \n        QuestionId_Answer.append(qid)\n        MisconceptionId.append(cid)\n        scores.append(score)\n    \n    result_df = pd.DataFrame()\n    result_df[\"QuestionId_Answer\"] = QuestionId_Answer\n    result_df[\"MisconceptionId\"] = MisconceptionId\n    result_df[\"score\"] = scores\n    \n    agg_df = result_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n    score_agg_df = result_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n    agg_df = pd.merge(agg_df, score_agg_df, on=\"QuestionId_Answer\", how=\"left\")\n    \n    agg_df[\"topk_info\"] = agg_df.apply(lambda x: sort_by_scores(x[\"MisconceptionId\"], x[\"score\"]), axis=1)\n    agg_df[\"MisconceptionId\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_ids\"])\n    agg_df[\"score\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_scores\"])\n    \n    # compute oof dataframe ---\n    oof_df = agg_df.copy()\n    oof_df = oof_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n    oof_df = oof_df.rename(columns={\"score\": \"logit_scores\"})\n\n    # normalize ---\n    oof_df[\"pred_scores\"] = oof_df[\"logit_scores\"].apply(stable_softmax)\n    oof_df[\"MisconceptionId\"] = oof_df[\"MisconceptionId\"].apply(lambda x: list(map(str, x)))\n\n    # print ---\n    print(\"--\"*40)\n    row = oof_df.sample()\n    formatted_scores = [f\"{s:.3f}\" for s in row['pred_scores'].values[0]]\n    misconceptions = row['MisconceptionId'].values[0]\n    print(f\"Showing 1 example: {row['QuestionId_Answer'].values[0]}\")\n    for rank, (m, s) in enumerate(zip(misconceptions, formatted_scores)):\n        print(f\"MisconceptionId: {m} -> Score: {s}\")\n    print(\"--\"*40)\n\n    save_path = os.path.join(save_dir, f\"ranker_{model_id}.parquet\")\n    oof_df.to_parquet(save_path)\n\nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument('--config_path', type=str, required=True)\n    ap.add_argument('--save_dir', type=str, required=True)\n    ap.add_argument('--model_id', type=str, required=True)\n\n    args = ap.parse_args()\n    cfg = OmegaConf.load(args.config_path)\n\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    # execution ---\n    main(cfg, save_dir=args.save_dir, model_id=args.model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:39:38.413889Z","iopub.execute_input":"2024-12-07T21:39:38.414299Z","iopub.status.idle":"2024-12-07T21:39:38.4241Z","shell.execute_reply.started":"2024-12-07T21:39:38.414266Z","shell.execute_reply":"2024-12-07T21:39:38.423007Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Qwen 32b Configs","metadata":{}},{"cell_type":"code","source":"%%writefile conf_oracle_32b_cv663_ff.yaml\n\nseed: 4562\nk_shot: 0\nuse_cot: true\n\ninput_path: ./ranker_outputs/ranker_input_stage_two.parquet\nicl_path: /kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\ncot_path: ./gen/gen_qwen_14b.parquet\n\nmodel:\n    backbone_path: \"/kaggle/input/eedi-oracle-32b-cv663-dec7-awq-ff/transformers/default/1\"\n    max_length: 768\n    num_proc: 2\n    \n    tokenizer:\n        padding_side: left\n        truncation_side: left\n        use_fast: true","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:39:52.816878Z","iopub.execute_input":"2024-12-07T21:39:52.817642Z","iopub.status.idle":"2024-12-07T21:39:52.823122Z","shell.execute_reply.started":"2024-12-07T21:39:52.817606Z","shell.execute_reply":"2024-12-07T21:39:52.822202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Qwen32b Infer","metadata":{}},{"cell_type":"code","source":"%%time\n!python run_sonnet_v2.py --config_path \"./conf_oracle_32b_cv663_ff.yaml\" --save_dir \"./ranker_outputs\" --model_id \"qwen_32b_oracle_main\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:39:56.61575Z","iopub.execute_input":"2024-12-07T21:39:56.616126Z","iopub.status.idle":"2024-12-07T21:46:50.891297Z","shell.execute_reply.started":"2024-12-07T21:39:56.616081Z","shell.execute_reply":"2024-12-07T21:46:50.89023Z"},"scrolled":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.4 Prep for stage 4","metadata":{}},{"cell_type":"code","source":"%%writefile blend_two_three.py\n\nimport argparse\nimport os\nfrom copy import deepcopy\n\nimport pandas as pd\nimport numpy as np\nfrom omegaconf import OmegaConf\n\ndef get_sorted_pairs(content_ids, scores):\n    collection = [(cid, s) for cid, s in zip(content_ids, scores)]\n    sorted_collection = sorted(collection, key=lambda x: x[1], reverse=True)\n    return sorted_collection\n\ndef cut_at_n(sub_df, n=25):\n    sub_df[\"MisconceptionId\"] = sub_df[\"MisconceptionId\"].apply(lambda x: x[:n])\n    sub_df[\"score\"] = sub_df[\"score\"].apply(lambda x: x[:n])\n    return sub_df\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config-path\", type=str)\n    args = parser.parse_args()\n    \n    with open(args.config_path, \"r\") as f:\n        cfg = OmegaConf.load(f)\n\n    # read predictions ---\n    haiku_preds = pd.read_parquet(cfg.haiku_path)\n    sonnet_preds = pd.read_parquet(cfg.sonnet_path)\n    \n    print(\"Sample Haiku Preds:\")\n    print(haiku_preds.sample(3))\n    print(\"--\"*50)\n    print(\"Sample Sonnet Preds:\")\n    print(sonnet_preds.sample(3))\n    print(\"--\"*50)\n\n    # flatten ---\n    haiku_preds = haiku_preds[[\"QuestionId_Answer\", \"MisconceptionId\", \"pred_scores\"]].explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n    haiku_preds = haiku_preds.rename(columns={\"pred_scores\": \"score_haiku\"})\n    haiku_preds[\"MisconceptionId\"] = haiku_preds[\"MisconceptionId\"].astype(str)\n\n    sonnet_preds = sonnet_preds[[\"QuestionId_Answer\", \"MisconceptionId\", \"pred_scores\"]].explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n    sonnet_preds = sonnet_preds.rename(columns={\"pred_scores\": \"score_sonnet\"})\n    sonnet_preds[\"MisconceptionId\"] = sonnet_preds[\"MisconceptionId\"].astype(str)\n\n    # blend ---\n    w_haiku = cfg.haiku_weight\n    w_sonnet = cfg.sonnet_weight\n\n    candidate_df = pd.merge(haiku_preds, sonnet_preds, on=[\"QuestionId_Answer\", \"MisconceptionId\"])\n    candidate_df[\"score\"] = candidate_df.apply(lambda x: w_haiku*x['score_haiku'] + w_sonnet*x['score_sonnet'], axis=1) # blending\n    candidate_df = candidate_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n\n    cdf = candidate_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n    sdf = candidate_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n    candidate_df = pd.merge(cdf, sdf, on=\"QuestionId_Answer\", how=\"left\")\n\n    candidate_df[\"sorted\"] = candidate_df.apply(lambda x: get_sorted_pairs(x['MisconceptionId'], x['score']), axis=1)\n    candidate_df[\"MisconceptionId\"] = candidate_df[\"sorted\"].apply(lambda x: [y[0] for y in x])\n    candidate_df[\"score\"] = candidate_df[\"sorted\"].apply(lambda x: [y[1] for y in x])\n    candidate_df = candidate_df.drop(columns=['sorted'])\n    \n    print(\"--\"*40)\n    print(f\"Saving sonnet+haiku prediction to: {cfg.blended_pred_path}\")\n    candidate_df.to_parquet(cfg.blended_pred_path)\n    \n    print(\"Example:\")\n    print(candidate_df.sample().T)\n    print(\"--\"*40)\n    print(\"Distribution:\")\n    print(candidate_df['MisconceptionId'].apply(len).value_counts())\n    print(\"--\"*40)\n\n    # Cut at N ---\n    candidate_df = cut_at_n(candidate_df, n=cfg.cutoff_n)\n    candidate_df.to_parquet(cfg.tutor_base_path)\n    \n    input_df = pd.read_parquet(cfg.ranker_input_file)\n    \n    print(f\"Shape of ranker input previously: {input_df.shape}\")\n    keep_df = candidate_df[['QuestionId_Answer', 'MisconceptionId']].explode(\"MisconceptionId\").reset_index(drop=True)\n    keep_df['MisconceptionId'] = keep_df['MisconceptionId'].astype(input_df[\"MisconceptionId\"].dtype)\n    input_df = input_df.merge(keep_df, on=[\"QuestionId_Answer\", \"MisconceptionId\"], how=\"inner\")\n    print(f\"shape of ranker input for stage 4: {input_df.shape}\")\n\n    # # Prepare further ranking ---\n    save_path = cfg.stage4_input_path\n    print(f\"saving output to: {save_path}\")\n    input_df.to_parquet(save_path)\n    print(f\"shape of input_df output: {input_df.shape}\")\n    print(\"--\"*40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:46:57.593226Z","iopub.execute_input":"2024-12-07T21:46:57.593708Z","iopub.status.idle":"2024-12-07T21:46:57.601775Z","shell.execute_reply.started":"2024-12-07T21:46:57.593663Z","shell.execute_reply":"2024-12-07T21:46:57.600905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile two_three_blend.yaml\n\nranker_input_file: ./retriever_outputs/ranker_input_stage_one.parquet\n\nhaiku_path: ./ranker_outputs/ranker_qwen_14b.parquet\nsonnet_path: ./ranker_outputs/ranker_qwen_32b_oracle_main.parquet\n\nhaiku_weight: 1.0\nsonnet_weight: 2.0\n\ncutoff_n: 5\n\nblended_pred_path: ./ranker_outputs/two_three_blended.parquet\ntutor_base_path: ./ranker_outputs/tutor_base.parquet\nstage4_input_path: ./ranker_outputs/ranker_input_stage_four.parquet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:46:58.526463Z","iopub.execute_input":"2024-12-07T21:46:58.527304Z","iopub.status.idle":"2024-12-07T21:46:58.532763Z","shell.execute_reply.started":"2024-12-07T21:46:58.527271Z","shell.execute_reply":"2024-12-07T21:46:58.531876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python blend_two_three.py --config-path two_three_blend.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:46:59.478834Z","iopub.execute_input":"2024-12-07T21:46:59.479716Z","iopub.status.idle":"2024-12-07T21:47:01.830932Z","shell.execute_reply.started":"2024-12-07T21:46:59.479674Z","shell.execute_reply":"2024-12-07T21:47:01.829991Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.5 Tutor Data Prep","metadata":{}},{"cell_type":"code","source":"%%writefile prep_tutor_data.py\n\nimport argparse\nimport os\nfrom copy import deepcopy\n\nimport pandas as pd\nimport numpy as np\nfrom omegaconf import OmegaConf\n\ndef eedi_process_df(df):\n    df = deepcopy(df)\n    df = df.rename(columns={\"QuestionId\": \"query_id\"})\n    grouped = df.groupby(\"query_id\")\n\n    question_dict = {}\n    for question_id, group in grouped:\n        question_data = group.to_dict(orient=\"records\")[0]\n        del question_data[\"query_id\"]\n        question_dict[question_id] = question_data\n\n    all_questions = list(question_dict.keys())\n\n    queries = []\n    for qid in all_questions:\n        info = question_dict[qid]\n\n        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n            if info[\"CorrectAnswer\"] == answer_key:\n                continue\n\n            this_example = dict()\n            this_key = f\"{qid}_{answer_key}\"\n            this_example[\"query_id\"] = this_key\n\n            for col in [\"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n                this_example[col] = info[col]\n\n            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n            queries.append(this_example)\n    # --\n    query_df = pd.DataFrame(queries)\n    return query_df\n    \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config-path\", type=str)\n    args = parser.parse_args()\n    \n    with open(args.config_path, \"r\") as f:\n        cfg = OmegaConf.load(f)\n\n\n    # read data ---\n    test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n    if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        n_ex = int(os.getenv(\"N_EX\"))\n        test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").head(n_ex)\n    content_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n    id2name = dict(zip(content_df['MisconceptionId'], content_df['MisconceptionName']))\n\n    query_df = eedi_process_df(test_df)\n\n    # read predictions ---\n    oof_df = pd.read_parquet(cfg.tutor_base_path)\n    oof_df[\"MisconceptionId\"] = oof_df[\"MisconceptionId\"].apply(lambda x: list(map(int, x)))\n    oof_df = oof_df.rename(columns={'QuestionId_Answer': 'query_id', 'MisconceptionId': 'content_ids'})\n    oof_df = oof_df[['query_id', 'content_ids']].copy()\n\n    # cots\n    cot_df_7b = pd.read_parquet(cfg.cot_7b_path)\n    cot_df_7b = cot_df_7b[['query_id', 'cot']].rename(columns={'cot': 'cot_7b'})\n\n    cot_df_14b = pd.read_parquet(cfg.cot_14b_path)\n    cot_df_14b = cot_df_14b[['query_id', 'cot']].rename(columns={'cot': 'cot_14b'})\n    \n\n    cot_df_32b = pd.read_parquet(cfg.cot_32b_path)\n    cot_df_32b = cot_df_32b[['query_id', 'cot']].rename(columns={'cot': 'cot_32b'})\n\n    # prep\n    query_df = query_df.merge(oof_df, on=\"query_id\", how=\"left\")\n    query_df['MisconceptionNameList'] = query_df['content_ids'].apply(lambda x: [id2name[y] for y in x])\n\n    query_df = query_df.merge(cot_df_7b, on='query_id', how='left')\n    query_df = query_df.merge(cot_df_14b, on='query_id', how='left')\n    query_df = query_df.merge(cot_df_32b, on='query_id', how='left')\n\n    print(query_df[\"cot_7b\"].isna().sum())\n    print(query_df[\"cot_14b\"].isna().sum())\n    print(query_df[\"cot_32b\"].isna().sum())\n\n    query_df[\"cot_7b\"] = query_df[\"cot_7b\"].fillna(\"\")\n    query_df[\"cot_14b\"] = query_df[\"cot_14b\"].fillna(\"\")\n    query_df[\"cot_32b\"] = query_df[\"cot_32b\"].fillna(\"\")\n\n    final_df = query_df[\n        [\"query_id\", \"content_ids\", \"SubjectName\", \"ConstructName\", \"QuestionText\", \n         \"CorrectAnswerText\", \"InCorrectAnswerText\", \"MisconceptionNameList\", \"cot_7b\", \"cot_14b\", \"cot_32b\"]\n    ].copy()\n    \n    print(\"--\"*50)\n    final_df = final_df.sort_values(by=\"query_id\").reset_index(drop=True)\n    ex = final_df.sample().to_dict(orient='records')[0]\n    for k, v in ex.items():\n        print(f\"{k} -> {v}\")\n\n    print(f\"saving tutor data to: {cfg.tutor_input_path}\")\n    final_df.to_parquet(cfg.tutor_input_path)\n    print(\"--\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:47:10.86008Z","iopub.execute_input":"2024-12-07T21:47:10.860958Z","iopub.status.idle":"2024-12-07T21:47:10.868949Z","shell.execute_reply.started":"2024-12-07T21:47:10.860919Z","shell.execute_reply":"2024-12-07T21:47:10.867905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile tutor_data_prep.yaml\n\ntutor_base_path: ./ranker_outputs/tutor_base.parquet\n\ncot_7b_path: ./gen/gen_qwen_7b.parquet\ncot_14b_path: ./gen/gen_qwen_14b.parquet\ncot_32b_path: ./gen/gen_qwen_32b.parquet\n\ntutor_input_path: ./ranker_outputs/tutor_input.parquet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:47:13.047757Z","iopub.execute_input":"2024-12-07T21:47:13.048113Z","iopub.status.idle":"2024-12-07T21:47:13.053927Z","shell.execute_reply.started":"2024-12-07T21:47:13.048081Z","shell.execute_reply":"2024-12-07T21:47:13.05311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python prep_tutor_data.py --config-path tutor_data_prep.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T21:47:14.730166Z","iopub.execute_input":"2024-12-07T21:47:14.730912Z","iopub.status.idle":"2024-12-07T21:47:16.61146Z","shell.execute_reply.started":"2024-12-07T21:47:14.730876Z","shell.execute_reply":"2024-12-07T21:47:16.610601Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 4: Ranker (Opus)","metadata":{}},{"cell_type":"code","source":"%%writefile run_expert_tutor.py\n\nimport sys\n\nsys.path.insert(0, \"/kaggle/input/eedi-utils-v12\")\n\nimport argparse\nimport os\nimport random\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport vllm\nfrom llm_tutor.ranker_dataset import RankerDataset\nfrom omegaconf import OmegaConf\n\n\ndef is_nan(x):\n    return x != x\n\n\ndef eedi_process_df(df):\n    df = df.copy()\n    df = df.rename(columns={\"QuestionId\": \"query_id\"})\n    grouped = df.groupby(\"query_id\")\n\n    question_dict = {}\n    for question_id, group in grouped:\n        question_data = group.to_dict(orient=\"records\")[0]\n        del question_data[\"query_id\"]\n        question_dict[question_id] = question_data\n\n    all_questions = list(question_dict.keys())\n\n    queries = []\n    query2content = defaultdict(list)\n    content2query = defaultdict(list)\n\n    # ---\n    for qid in all_questions:\n        info = question_dict[qid]\n\n        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n            if info[\"CorrectAnswer\"] == answer_key:\n                continue\n            this_example = dict()\n            this_key = f\"{qid}_{answer_key}\"\n            this_example[\"query_id\"] = this_key\n\n            if is_nan(info[f\"Misconception{answer_key}Id\"]):\n                continue\n\n            mid = str(int(info[f\"Misconception{answer_key}Id\"]))\n            query2content[this_key].append(mid)\n            content2query[mid].append(this_key)\n\n            # ---\n            for col in [\"SubjectId\", \"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n                this_example[col] = info[col]\n\n            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n            this_example[\"AllOptionText\"] = \"\\n- \".join([info[f\"Answer{x}Text\"] for x in [\"A\", \"B\", \"C\", \"D\"]])\n            this_example[\"AllOptionText\"] = f\"\\n- {this_example['AllOptionText']}\"\n            queries.append(this_example)\n    # --\n    query_df = pd.DataFrame(queries)\n    corr_df = pd.Series(query2content).reset_index().rename(columns={\"index\": \"query_id\", 0: \"content_id\"})\n    corr_df[\"content_id\"] = corr_df[\"content_id\"].apply(lambda x: x[0])\n\n    query_df = query_df.reset_index(drop=True)\n\n    return query_df, corr_df, content2query\n\n\ndef stable_softmax(x, temp=1.0):\n    x = np.array(x) / temp\n    x_max = np.max(x)\n    exp_x = np.exp(x - x_max)\n    return exp_x / np.sum(exp_x)\n\n\ndef sort_by_scores(pred_ids, scores):\n    keep_idxs = np.argsort(-np.array(scores)).tolist()\n    ret_ids = [pred_ids[idx] for idx in keep_idxs]\n    ret_scores = [scores[idx] for idx in keep_idxs]\n    return {\"sorted_ids\": ret_ids, \"sorted_scores\": ret_scores}\n\n\ndef format_example(row, id2name, query2content):\n    cid = int(query2content[row[\"query_id\"]])\n    misconception_name = id2name[cid]\n    example = f\"Question: {row['QuestionText']}\\nAnswer:{row['CorrectAnswerText']}\\nIncorrect Answer: {row['InCorrectAnswerText']}\\nMisconception: {misconception_name}\"\n    return example\n\n\ndef add_fs_examples_tutor_for_eval(df, content2query, query2example, rng):\n    cache = {}\n\n    def _add_examples(row):\n        cids = row[\"content_ids\"]\n\n        selected_qids = []\n        for cid in cids:\n            cid = int(cid)\n            qids = content2query[cid]  # content2query is a defaultdict(list)\n            qids = [qid for qid in qids if qid != row[\"query_id\"]]\n            if len(qids) > 0:\n                if cid not in cache:\n                    selected = rng.choice(qids)\n                    cache[cid] = selected\n                    selected_qids.append(cache[cid])\n                else:\n                    selected_qids.append(cache[cid])\n\n        # print(f\"selected_qids: {selected_qids}\")\n        if len(selected_qids) == 0:\n            return \"\"\n\n        # keep max of 3 examples\n        selected_qids = rng.sample(selected_qids, k=min(3, len(selected_qids)))\n        selected_qids = sorted(selected_qids)  # for prefix cache\n        examples = [query2example[qid] for qid in selected_qids]\n        fs = \"\\n--\\n\".join(examples)\n        # print(f\"fs: {fs}\")\n        return fs\n\n    df[\"examples\"] = df.apply(_add_examples, axis=1)\n    return df\n\n\ndef main(cfg, save_dir, model_id):\n    test_df = pd.read_parquet(cfg.input_path)\n\n    if cfg.use_tta:\n        test_df_tta = test_df.copy()\n        test_df_tta[\"content_ids\"] = test_df_tta[\"content_ids\"].apply(lambda x: x[::-1])\n        test_df_tta[\"MisconceptionNameList\"] = test_df_tta[\"MisconceptionNameList\"].apply(lambda x: x[::-1])\n        test_df = pd.concat([test_df, test_df_tta]).reset_index(drop=True)\n        test_df = test_df.sort_values(by=\"query_id\").reset_index(drop=True)\n\n    # comp few shot examples ---\n    rng = random.Random(cfg.seed)\n\n    fs_df = pd.read_csv(cfg.icl_path).rename(columns={\"QuestionId\": \"query_id\"})\n    content_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n    id2name = dict(zip(content_df[\"MisconceptionId\"], content_df[\"MisconceptionName\"]))\n\n    query_df, fs_corr_df, content2query = eedi_process_df(fs_df)\n    fs_query2content = dict(zip(fs_corr_df[\"query_id\"], fs_corr_df[\"content_id\"]))\n    query_df[\"demo\"] = query_df.apply(lambda x: format_example(x, id2name, fs_query2content), axis=1)\n    query2example = dict(zip(query_df[\"query_id\"], query_df[\"demo\"]))\n\n    test_df = add_fs_examples_tutor_for_eval(test_df, content2query, query2example, rng)\n    print(f\"shape of test data: {test_df.shape}\")\n    print(\"--\" * 40)\n\n    # ---\n    dataset_creator = RankerDataset(cfg)\n    infer_ds = dataset_creator.get_dataset(test_df)\n    tokenizer = dataset_creator.tokenizer\n\n    a_tok_id = tokenizer(\"A\", add_special_tokens=False)[\"input_ids\"][-1]\n    b_tok_id = tokenizer(\"B\", add_special_tokens=False)[\"input_ids\"][-1]\n    c_tok_id = tokenizer(\"C\", add_special_tokens=False)[\"input_ids\"][-1]\n    d_tok_id = tokenizer(\"D\", add_special_tokens=False)[\"input_ids\"][-1]\n    e_tok_id = tokenizer(\"E\", add_special_tokens=False)[\"input_ids\"][-1]\n\n    print(f\">> EediRanker: A token id: {a_tok_id}\")\n    print(f\">> EediRanker: B token id: {b_tok_id}\")\n    print(f\">> EediRanker: C token id: {c_tok_id}\")\n    print(f\">> EediRanker: D token id: {d_tok_id}\")\n    print(f\">> EediRanker: E token id: {e_tok_id}\")\n\n    infer_ds = infer_ds.map(lambda example: {\"prompt\": tokenizer.decode(example[\"input_ids\"], skip_special_tokens=False)})\n\n    infer_qa_ids = infer_ds[\"query_id\"]\n    infer_mc_ids = infer_ds[\"content_ids\"]\n    prompts = infer_ds[\"prompt\"]\n\n    print(f\"# of requests: {len(prompts)}\")\n    print(f\"Example:\\n\\n{prompts[0]}\")\n    print(\"data preparation done...\")\n\n    # -- create the llm ----#\n    llm = vllm.LLM(\n        cfg.model.backbone_path,\n        # quantization=\"awq\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.99,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2048,\n        disable_log_stats=True,\n        cpu_offload_gb=8,\n        swap_space=1,\n        device=\"cuda\",\n        max_num_seqs=20,\n        # enable_prefix_caching=True\n    )\n\n    sampling_params = vllm.SamplingParams(n=1, top_p=0.8, logprobs=20, max_tokens=1, temperature=0.0, skip_special_tokens=False)\n    responses = llm.generate(prompts, sampling_params, use_tqdm=True)\n    print(\"inference done...\")\n\n    # get results ---\n    print(\"--\" * 40)\n\n    QuestionId_Answer = []\n    MisconceptionId = []\n    scores = []\n\n    for qid, cids, response in zip(infer_qa_ids, infer_mc_ids, responses):\n        logprob_dict = response.outputs[0].logprobs[0]\n\n        top_tok_ids = set(list(logprob_dict.keys()))\n        if len(top_tok_ids.intersection(set([a_tok_id, b_tok_id, c_tok_id, d_tok_id, e_tok_id]))) == 0:\n            print(f\"Bad Output for {qid}\")\n            continue\n\n        a_logit, b_logit, c_logit, d_logit, e_logit = -10.0, -10.0, -10.0, -10.0, -10.0\n\n        if a_tok_id in logprob_dict:\n            a_logit = logprob_dict[a_tok_id].logprob\n\n        if b_tok_id in logprob_dict:\n            b_logit = logprob_dict[b_tok_id].logprob\n\n        if c_tok_id in logprob_dict:\n            c_logit = logprob_dict[c_tok_id].logprob\n\n        if d_tok_id in logprob_dict:\n            d_logit = logprob_dict[d_tok_id].logprob\n\n        if e_tok_id in logprob_dict:\n            e_logit = logprob_dict[e_tok_id].logprob\n\n        logits = np.array([a_logit, b_logit, c_logit, d_logit, e_logit])\n        logits_max = np.max(logits)\n        exp_logits = np.exp(logits - logits_max)\n        normalized_scores = exp_logits / np.sum(exp_logits)\n\n        QuestionId_Answer.append(qid)\n        MisconceptionId.append(cids)\n        scores.append(normalized_scores)\n\n    result_df = pd.DataFrame()\n    result_df[\"QuestionId_Answer\"] = QuestionId_Answer\n    result_df[\"MisconceptionId\"] = MisconceptionId\n    result_df[\"MisconceptionId\"] = result_df[\"MisconceptionId\"].apply(lambda x: [str(y) for y in x])\n    result_df[\"score\"] = scores\n\n    # ----\n    if cfg.use_tta:\n        result_df = result_df.explode([\"MisconceptionId\", \"score\"]).reset_index(drop=True)\n        result_df = result_df.groupby([\"QuestionId_Answer\", \"MisconceptionId\"]).agg({\"score\": \"mean\"}).reset_index()\n\n        # regroup --\n        agg_df = result_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n        score_agg_df = result_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n        agg_df = pd.merge(agg_df, score_agg_df, on=\"QuestionId_Answer\", how=\"left\")\n        result_df = agg_df.copy()\n\n    # --------\n    agg_df = result_df.copy()\n    agg_df[\"topk_info\"] = agg_df.apply(lambda x: sort_by_scores(x[\"MisconceptionId\"], x[\"score\"]), axis=1)\n    agg_df[\"MisconceptionId\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_ids\"])\n    agg_df[\"score\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_scores\"])\n\n    # compute oof dataframe ---\n    oof_df = agg_df.copy()\n    oof_df = oof_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n    oof_df = oof_df.rename(columns={\"score\": \"logit_scores\"})\n\n    # normalize ---\n    oof_df[\"pred_scores\"] = oof_df[\"logit_scores\"].apply(stable_softmax)\n    oof_df[\"MisconceptionId\"] = oof_df[\"MisconceptionId\"].apply(lambda x: list(map(str, x)))\n\n    # print ---\n    print(\"--\" * 40)\n    row = oof_df.sample()\n    formatted_scores = [f\"{s:.3f}\" for s in row[\"pred_scores\"].values[0]]\n    misconceptions = row[\"MisconceptionId\"].values[0]\n    print(f\"Showing 1 example: {row['QuestionId_Answer'].values[0]}\")\n    for rank, (m, s) in enumerate(zip(misconceptions, formatted_scores)):\n        print(f\"MisconceptionId: {m} -> Score: {s}\")\n    print(\"--\" * 40)\n\n    save_path = os.path.join(save_dir, f\"ranker_{model_id}.parquet\")\n    oof_df.to_parquet(save_path)\n\n\nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--config_path\", type=str, required=True)\n    ap.add_argument(\"--save_dir\", type=str, required=True)\n    ap.add_argument(\"--model_id\", type=str, required=True)\n\n    args = ap.parse_args()\n    cfg = OmegaConf.load(args.config_path)\n\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    # execution ---\n    main(cfg, save_dir=args.save_dir, model_id=args.model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:01:15.205819Z","iopub.execute_input":"2024-12-07T22:01:15.206143Z","iopub.status.idle":"2024-12-07T22:01:15.217107Z","shell.execute_reply.started":"2024-12-07T22:01:15.206119Z","shell.execute_reply":"2024-12-07T22:01:15.216123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile conf_expert_tutor.yaml\n\nseed: 8798\nk_shot: 0\n\nuse_tta: false\n\ninput_path: ./ranker_outputs/tutor_input.parquet\nicl_path: /kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\n\nmodel:\n    backbone_path: \"/kaggle/input/eedi-tutor-72b-cv661-dec7-awq/transformers/default/1\"\n    max_length: 2048\n    num_proc: 2\n    \n    tokenizer:\n        padding_side: left\n        truncation_side: left\n        use_fast: true","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:01:15.990319Z","iopub.execute_input":"2024-12-07T22:01:15.990704Z","iopub.status.idle":"2024-12-07T22:01:15.996762Z","shell.execute_reply.started":"2024-12-07T22:01:15.99067Z","shell.execute_reply":"2024-12-07T22:01:15.995742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n!python run_expert_tutor.py --config_path \"./conf_expert_tutor.yaml\" --save_dir \"./ranker_outputs\" --model_id \"qwen_72b_tutor\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:01:17.30817Z","iopub.execute_input":"2024-12-07T22:01:17.309098Z","iopub.status.idle":"2024-12-07T22:12:04.356798Z","shell.execute_reply.started":"2024-12-07T22:01:17.309045Z","shell.execute_reply":"2024-12-07T22:12:04.354878Z"},"scrolled":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# time for 16 examples (tta, no prompt caching): Processed prompts: 100%|| 96/96 [04:45<00:00,  2.97s/it, est. speed input: 177.\n# time for 16 examples (tta, prompt caching)   : 100%|| 96/96 [03:54<00:00,  2.44s/it, est. speed input: 215.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.2 Additional 32b","metadata":{}},{"cell_type":"code","source":"%%writefile conf_oracle_32b_cv652.yaml\n\nseed: 9461\nk_shot: 0\nuse_cot: false\n\ninput_path: ./ranker_outputs/ranker_input_stage_four.parquet\nicl_path: /kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\ncot_path: ./gen/gen_qwen_14b.parquet\n\nmodel:\n    backbone_path: \"/kaggle/input/eedi-ranker-32b-cv655-nov24-custom-awq/transformers/default/1\"\n    max_length: 640\n    num_proc: 2\n    \n    tokenizer:\n        padding_side: left\n        truncation_side: left\n        use_fast: true","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:12:38.176602Z","iopub.execute_input":"2024-12-07T22:12:38.177488Z","iopub.status.idle":"2024-12-07T22:12:38.184573Z","shell.execute_reply.started":"2024-12-07T22:12:38.177425Z","shell.execute_reply":"2024-12-07T22:12:38.183625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%time\n# !python run_sonnet_v1.py --config_path \"./conf_oracle_32b_cv652.yaml\" --save_dir \"./ranker_outputs\" --model_id \"qwen_32b_oracle_support\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:12:41.579544Z","iopub.execute_input":"2024-12-07T22:12:41.580262Z","iopub.status.idle":"2024-12-07T22:12:41.584293Z","shell.execute_reply.started":"2024-12-07T22:12:41.580224Z","shell.execute_reply":"2024-12-07T22:12:41.5832Z"},"scrolled":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.3 Ensemble Rankers","metadata":{}},{"cell_type":"code","source":"%%writefile ensemble_rankers.py\n\nimport argparse\nimport os\nfrom copy import deepcopy\n\nimport pandas as pd\nimport numpy as np\nfrom omegaconf import OmegaConf\n\ndef get_sorted_pairs(content_ids, scores):\n    collection = [(cid, s) for cid, s in zip(content_ids, scores)]\n    sorted_collection = sorted(collection, key=lambda x: x[1], reverse=True)\n    return sorted_collection\n\ndef stable_softmax(x, temp=1.0):\n    x = np.array(x) / temp\n    x_max = np.max(x)\n    exp_x = np.exp(x - x_max)\n    return exp_x / np.sum(exp_x)\n    \ndef _load_df(pth, cutoff):\n    df = pd.read_parquet(pth)\n    # df[\"MisconceptionId\"] = df[\"MisconceptionId\"].apply(lambda x: x[:cutoff])\n    \n    print(f\"recomputing softmax for {pth}...\")\n    # df[\"logit_scores\"] = df[\"logit_scores\"].apply(lambda x: x[:cutoff])\n    df[\"pred_scores\"] = df[\"logit_scores\"].apply(stable_softmax) # recompute ---\n    print(df.sample().T)\n    print(\"--\"*40)\n\n    return df\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config-path\", type=str)\n    args = parser.parse_args()\n    \n    with open(args.config_path, \"r\") as f:\n        cfg = OmegaConf.load(f)\n\n    # read predictions ---\n    one_preds = _load_df(cfg.ranker_1, cutoff=cfg.cutoff_n)\n    two_preds = _load_df(cfg.ranker_2, cutoff=cfg.cutoff_n)\n\n\n    # flatten\n    one_preds = one_preds[[\"QuestionId_Answer\", \"MisconceptionId\", \"pred_scores\"]].explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n    one_preds = one_preds.rename(columns={\"pred_scores\": \"score_one\"})\n    one_preds[\"MisconceptionId\"] = one_preds[\"MisconceptionId\"].astype(str)\n    \n    two_preds = two_preds[[\"QuestionId_Answer\", \"MisconceptionId\", \"pred_scores\"]].explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n    two_preds = two_preds.rename(columns={\"pred_scores\": \"score_two\"})\n    two_preds[\"MisconceptionId\"] = two_preds[\"MisconceptionId\"].astype(str)\n\n\n    # blend ---\n    w1, w2 = cfg.r1_weight, cfg.r2_weight\n\n    candidate_df = pd.merge(one_preds, two_preds, on=[\"QuestionId_Answer\", \"MisconceptionId\"])\n    print(candidate_df.sample().T)\n\n    candidate_df[\"score\"] = candidate_df.apply(lambda x: w1*x['score_one'] + w2*x['score_two'], axis=1) # blending\n    candidate_df = candidate_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n\n    cdf = candidate_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n    sdf = candidate_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n    candidate_df = pd.merge(cdf, sdf, on=\"QuestionId_Answer\", how=\"left\")\n\n    candidate_df[\"sorted\"] = candidate_df.apply(lambda x: get_sorted_pairs(x['MisconceptionId'], x['score']), axis=1)\n    candidate_df[\"MisconceptionId\"] = candidate_df[\"sorted\"].apply(lambda x: [y[0] for y in x])\n    candidate_df[\"score\"] = candidate_df[\"sorted\"].apply(lambda x: [y[1] for y in x])\n\n    # Cut at N ---\n    print(\"Sample:\")\n    candidate_df = candidate_df.drop(columns=['sorted'])\n    print(candidate_df.sample().T)\n\n    # Updated input df for re-ranking ---\n    ranked_df = candidate_df[[\"QuestionId_Answer\", \"MisconceptionId\"]].copy()\n    ranked_df = ranked_df.reset_index(drop=True)\n    \n    print(\"--\"*40)\n    ranked_df.to_parquet(cfg.outfile_path)\n    print(ranked_df.sample(3))\n    print(\"--\"*40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:13:00.055324Z","iopub.execute_input":"2024-12-07T22:13:00.056189Z","iopub.status.idle":"2024-12-07T22:13:00.063082Z","shell.execute_reply.started":"2024-12-07T22:13:00.056152Z","shell.execute_reply":"2024-12-07T22:13:00.062023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile ensemble_rankers.yaml\n\ncutoff_n: 5 # 25 # TODO <- FIX\n\nranker_1: ./ranker_outputs/ranker_qwen_72b_tutor.parquet\nranker_2: ./ranker_outputs/ranker_qwen_32b_oracle_main.parquet\n\nr1_weight: 4.0\nr2_weight: 1.0\n\noutfile_path: ./ranker_outputs/ranker_blend_stage4.parquet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:13:26.859037Z","iopub.execute_input":"2024-12-07T22:13:26.859797Z","iopub.status.idle":"2024-12-07T22:13:26.865363Z","shell.execute_reply.started":"2024-12-07T22:13:26.859758Z","shell.execute_reply":"2024-12-07T22:13:26.864284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python ensemble_rankers.py --config-path ensemble_rankers.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:13:27.346961Z","iopub.execute_input":"2024-12-07T22:13:27.3475Z","iopub.status.idle":"2024-12-07T22:13:29.811818Z","shell.execute_reply.started":"2024-12-07T22:13:27.347456Z","shell.execute_reply":"2024-12-07T22:13:29.810966Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npd.options.display.max_colwidth = None\n\ntop_df = pd.read_parquet(\"./ranker_outputs/ranker_blend_stage4.parquet\").rename(columns={\"MisconceptionId\": \"top_ids\"})\nmid_df = pd.read_parquet(\"./ranker_outputs/two_three_blended.parquet\").rename(columns={\"MisconceptionId\": \"mid_ids\"}).drop(columns=['score'])\nlow_df = pd.read_parquet(\"./ranker_outputs/one_two_blended.parquet\").rename(columns={\"MisconceptionId\": \"low_ids\"}).drop(columns=['score'])\n\npred_df = pd.merge(top_df, mid_df, on=[\"QuestionId_Answer\"])\npred_df = pd.merge(pred_df, low_df, on=[\"QuestionId_Answer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:13:30.931104Z","iopub.execute_input":"2024-12-07T22:13:30.93168Z","iopub.status.idle":"2024-12-07T22:13:31.34226Z","shell.execute_reply.started":"2024-12-07T22:13:30.931643Z","shell.execute_reply":"2024-12-07T22:13:31.341218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_final_ids(row):\n    def _cast_to_str(x): return [str(y) for y in x]\n        \n    top_ids = _cast_to_str(row['top_ids'])\n    mid_ids = _cast_to_str(row['mid_ids'])\n    low_ids = _cast_to_str(row['low_ids'])\n\n    ret = list(top_ids)\n    \n    for this_id in mid_ids: # add mid ids\n        if this_id not in ret: ret.append(this_id)\n\n    for this_id in low_ids: # add mid ids\n        if this_id not in ret: ret.append(this_id)\n\n    return ret","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:13:31.346632Z","iopub.execute_input":"2024-12-07T22:13:31.347156Z","iopub.status.idle":"2024-12-07T22:13:31.352248Z","shell.execute_reply.started":"2024-12-07T22:13:31.347127Z","shell.execute_reply":"2024-12-07T22:13:31.35129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_df[\"MisconceptionId\"] = pred_df.apply(get_final_ids, axis=1)\npred_df[\"MisconceptionId\"] = pred_df[\"MisconceptionId\"].apply(lambda x: x[:25])\npred_df[\"MisconceptionId\"] = pred_df[\"MisconceptionId\"].apply(lambda x: \" \".join(x))\nsub_df = pred_df[[\"QuestionId_Answer\", \"MisconceptionId\"]].copy()\nsub_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:13:31.571984Z","iopub.execute_input":"2024-12-07T22:13:31.57268Z","iopub.status.idle":"2024-12-07T22:13:31.593113Z","shell.execute_reply.started":"2024-12-07T22:13:31.572647Z","shell.execute_reply":"2024-12-07T22:13:31.592191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub_df = pd.read_csv(\"submission.csv\")\nsub_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:13:32.062776Z","iopub.execute_input":"2024-12-07T22:13:32.063149Z","iopub.status.idle":"2024-12-07T22:13:32.078761Z","shell.execute_reply.started":"2024-12-07T22:13:32.063106Z","shell.execute_reply":"2024-12-07T22:13:32.077994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# End ---#","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T22:13:32.683098Z","iopub.execute_input":"2024-12-07T22:13:32.6834Z","iopub.status.idle":"2024-12-07T22:13:32.687744Z","shell.execute_reply.started":"2024-12-07T22:13:32.683371Z","shell.execute_reply":"2024-12-07T22:13:32.686817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}