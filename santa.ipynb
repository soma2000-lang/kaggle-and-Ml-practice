{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":10628826,"sourceType":"datasetVersion","datasetId":6580948},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Simulated Annealing with Custom Weighted Candidate Generator\n","metadata":{}},{"cell_type":"markdown","source":"# This Solution:\n\n- Works best with Sample 0 to 4\n- With correct starting text achives 30.x on Sample 5\n- @[siavrez](https://www.kaggle.com/siavrez) code that reached 28.5x on Sample 5 can be found here :\n  [Solution 2](https://www.kaggle.com/code/siavrez/3rd-place-sa-weighted-candidate-generator-sol2)","metadata":{}},{"cell_type":"markdown","source":"# Batch Metric","metadata":{}},{"cell_type":"code","source":"\"\"\"Evaluation metric for Santa 2024.\"\"\"\n\nimport gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = False,\n    clear_mem: bool = False,\n) -> float:\n    \"\"\"\n    Calculates the mean perplexity of submitted text permutations compared to an original text.\n\n    Parameters\n    ----------\n    solution : DataFrame\n        DataFrame containing the original text in a column named 'text'.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    submission : DataFrame\n        DataFrame containing the permuted text in a column named 'text'.\n        Must have the same row IDs as the solution.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    row_id_column_name : str\n        Name of the column containing row IDs.\n        Ensures aligned comparison between solution and submission.\n\n    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n        Path to the serialized LLM.\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    clear_mem : bool, default=False\n        Clear GPU memory after scoring by clearing the CUDA cache.\n        Useful for testing.\n\n    Returns\n    -------\n    float\n        The mean perplexity score. Lower is better.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the submission format is invalid or submitted strings are not valid permutations.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n    ... })\n    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n    True\n    \"\"\"\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculatorBatches:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        #self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path,padding_side=\"right\")\n\n\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            print(\"Load small)\")\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n                #attn_implementation=\"sdpa\",\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n    def get_perplexity( self, input_texts: Union[str, List[str]], batch_size: 32) -> Union[float, List[float]]:\n\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n\n        batches = len(input_texts)//batch_size + (len(input_texts)%batch_size != 0)\n        for j in range(batches):\n\n            a = j*batch_size\n            b = (j+1)*batch_size\n            input_batch = input_texts[a:b]\n\n            with torch.no_grad():\n\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = [f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\" for text in input_batch]\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                    padding=True\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                label = model_inputs['input_ids']\n                label[label == self.tokenizer.pad_token_id] = PAD_TOKEN_LABEL_ID\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = label[..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                loss = loss.view(len(logits), -1)\n                valid_length = (shift_labels != PAD_TOKEN_LABEL_ID).sum(dim=-1)\n                loss = torch.sum(loss, -1) / valid_length\n\n                loss_list += loss.cpu().tolist()\n\n        ppl = [exp(i) for i in loss_list]\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n#871 or 178\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T16:53:16.087642Z","iopub.execute_input":"2025-01-31T16:53:16.087883Z","iopub.status.idle":"2025-01-31T16:53:24.471697Z","shell.execute_reply.started":"2025-01-31T16:53:16.087862Z","shell.execute_reply":"2025-01-31T16:53:24.470765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scorer = PerplexityCalculatorBatches(\"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T16:53:24.472597Z","iopub.execute_input":"2025-01-31T16:53:24.473099Z","iopub.status.idle":"2025-01-31T16:56:56.511814Z","shell.execute_reply.started":"2025-01-31T16:53:24.473073Z","shell.execute_reply":"2025-01-31T16:56:56.510883Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"# CandidateGenerator\n\n- This class is responsible for generating candidate texts to score.\n- We defined different operations (text transformation operations).\n- The selection of which operation to use is not purely random, we created a weighting system to give importance to operations based on the score they produce.","metadata":{}},{"cell_type":"code","source":"import random\nimport math\nimport time\nfrom collections import deque\n\n\n\nclass CandidateGenerator:\n    def __init__(self, scorer, batch_size=32):\n        self.global_cache = set()\n        self.max_cache_size = 1000000000  # Define max cache size to avoid memory bloat\n        self.operations = [ # The selection of operations is based on the sample/ many tires to reach best selections \n            \"swap\",\n            \"reverse\",\n            \"removeinsert\",\n            #\"removeinsertn\",\n            \"rotate\",\n            \"shift\",\n            \"swap_adjacent\",\n            \"circular_shift\",\n            #\"reverse_all\",\n            #\"scramble_except_first\",\n            \"block_shift\",\n            \"block_swap\",\n        ]\n        self.operation_weights = [1.0] * len(self.operations)  # Initial equal weights\n\n        # Initialize word importance scores\n        self.word_importance = {}\n        self.scorer = scorer\n        self.batch_size = batch_size\n\n    def compute_word_importance(self, words):\n        \"\"\"\n        Compute word importance based on perplexity scores.\n        \"\"\"\n        importance_scores = {}\n        for word in words:\n            # Get perplexity for each word\n            perplexity = self.scorer.get_perplexity([word], batch_size=self.batch_size)\n            importance_scores[word] = 1 / perplexity[0]  # Higher perplexity -> Lower importance\n        self.word_importance = importance_scores\n\n    def targeted_modify_text_batch(self, text, num_modifications):\n        words = text.split()\n        candidates = []\n\n        # Normalize weights to create a probability distribution\n        total_weight = sum(self.operation_weights)\n        operation_probs = [w / total_weight for w in self.operation_weights]\n\n        for _ in range(num_modifications):\n            operation = random.choices(self.operations, weights=operation_probs, k=1)[0]\n            new_words = words[:]\n\n            # Incorporate word importance into operations\n            high_importance_words = [\n                idx for idx, word in enumerate(new_words) if self.word_importance.get(word, 0) > 0.5\n            ]\n\n            if operation == \"removeinsert\":\n                idx1 = random.choice(high_importance_words) if high_importance_words else random.randint(0, len(new_words) - 1)\n                word = new_words.pop(idx1)\n                idx2 = random.randint(0, len(new_words))\n                new_words.insert(idx2, word)\n            if operation == \"removeinsertn\":\n                # Determine the number of words to remove and reinsert (N between 1 and 4)\n                N = random.randint(1, 4)\n\n                # Ensure N does not exceed the length of the list\n                N = min(N, len(new_words))\n\n                for _ in range(N):\n                  # Select a word to remove\n                  idx1 = random.randint(0, len(new_words) - 1)\n                  word = new_words.pop(idx1)\n\n                  # Select a new position to insert\n                  idx2 = random.randint(0, len(new_words))\n                  new_words.insert(idx2, word)\n            elif operation == \"swap\":\n                if high_importance_words:\n                    idx1 = random.choice(high_importance_words)\n                    idx2 = random.randint(0, len(new_words) - 1)\n                else:\n                    idx1, idx2 = random.sample(range(len(new_words)), 2)\n                new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n            elif operation == \"reverse\":\n                idx1, idx2 = sorted(random.sample(range(len(new_words)), 2))\n                new_words[idx1:idx2] = reversed(new_words[idx1:idx2])\n            elif operation == \"rotate\":\n                idx1, idx2 = sorted(random.sample(range(len(new_words)), 2))\n                if idx2 - idx1 > 1:\n                    subarray = new_words[idx1:idx2]\n                    rotation = random.randint(1, len(subarray) - 1)\n                    new_words[idx1:idx2] = subarray[rotation:] + subarray[:rotation]\n            elif operation == \"shift\":\n                idx1, idx2 = sorted(random.sample(range(len(new_words)), 2))\n                if idx2 - idx1 > 1:\n                    subarray = new_words[idx1:idx2]\n                    shift = random.choice([-1, 1])\n                    new_words[idx1:idx2] = subarray[shift:] + subarray[:shift]\n            elif operation == \"swap_adjacent\":\n                idx = random.randint(0, len(new_words) - 2)\n                new_words[idx], new_words[idx + 1] = new_words[idx + 1], new_words[idx]\n            elif operation == \"circular_shift\":\n                shift = random.randint(1, len(new_words) - 1)\n                new_words = new_words[shift:] + new_words[:shift]\n            elif operation == \"reverse_all\":\n                new_words = list(reversed(new_words))\n            elif operation == \"scramble_except_first\":\n                first_word = new_words[0]\n                rest = new_words[1:]\n                random.shuffle(rest)\n                new_words = [first_word] + rest\n            elif operation == \"block_shift\":\n                idx1, idx2 = sorted(random.sample(range(len(new_words)), 2))\n                block = new_words[idx1:idx2]\n                del new_words[idx1:idx2]\n                insert_idx = random.randint(0, len(new_words))\n                new_words[insert_idx:insert_idx] = block\n            elif operation == \"block_swap\":\n                idx1, idx2 = sorted(random.sample(range(len(new_words)), 2))\n                idx3, idx4 = sorted(random.sample(range(len(new_words)), 2))\n                if idx1 < idx2 and idx3 < idx4 and idx1 != idx3:\n                    block1 = new_words[idx1:idx2]\n                    block2 = new_words[idx3:idx4]\n                    new_words[idx1:idx2], new_words[idx3:idx4] = block2, block1\n\n            # Ensure the length of the new text matches the original and is a valid permutation\n            if len(new_words) != len(words) or sorted(new_words) != sorted(words):\n                continue\n\n            candidate = ' '.join(new_words)\n\n            # Add to global cache if unique\n            if candidate not in self.global_cache:\n                candidates.append((candidate, operation))  # Track operation for reward-punishment\n                self.global_cache.add(candidate)\n\n                # Clear cache if size exceeds limit\n                if len(self.global_cache) > self.max_cache_size:\n                    self.global_cache.clear()\n                    print(\"Cache Restarted ....\")\n\n        #print(f\"Current Cache has {len(self.global_cache)}\")\n\n        return candidates\n\n    def update_weights(self, operation_scores):\n        \"\"\"\n        Updates the weights for operations based on their performance.\n        :param operation_scores: A dictionary mapping operations to their scores.\n        \"\"\"\n        min_score = min(operation_scores.values())\n        max_score = max(operation_scores.values())\n\n        # Reward-punishment mechanism\n        for i, operation in enumerate(self.operations):\n            score = operation_scores.get(operation, max_score)\n            if score == min_score:\n                self.operation_weights[i] *= 1.2  # Strong reward for best operation\n            else:\n                penalty = 0.9 + (0.1 * (score - min_score) / (max_score - min_score + 1e-6))\n                self.operation_weights[i] *= penalty\n\n        # Normalize weights to avoid extreme bias\n        total_weight = sum(self.operation_weights)\n        if total_weight <= 0 or not math.isfinite(total_weight):\n            # Reset weights to equal distribution if normalization fails\n            self.operation_weights = [1.0] * len(self.operation_weights)\n        else:\n            self.operation_weights = [w / total_weight for w in self.operation_weights]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T16:56:56.513359Z","iopub.execute_input":"2025-01-31T16:56:56.51394Z","iopub.status.idle":"2025-01-31T16:56:56.532655Z","shell.execute_reply.started":"2025-01-31T16:56:56.513915Z","shell.execute_reply":"2025-01-31T16:56:56.532026Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Enhanced Simulated Annealing:\n\n- Tracking operations (text transformation operations score and use that to update the selection of candidates.\n- Taking texts with \"equal best score\" into consideration, this help escape dead-end paths.\nÂ  Note: This happens with A100 GPUs at least, where two texts (or more) can have the same score.\n- Trying to fix a prefix/postfix part of the text and optimize other parts (didn't really work)\n- No improvements: when no improvements happen we reselect a text and start from (basically some shuffling or back to the best text), it's based on experiments.\n","metadata":{}},{"cell_type":"code","source":"class EnhancedSimulatedAnnealing:\n    def __init__(self, scorer, initial_text, initial_temperature, cooling_rate, max_iterations, max_time, batch_size=32):\n        \"\"\"\n        Initializes the optimizer.\n        \"\"\"\n        self.scorer = scorer\n        self.initial_words = initial_text.split()\n        self.temperature = initial_temperature\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n        self.max_iterations = max_iterations\n        self.max_time = max_time\n        self.batch_size = batch_size\n        self.start_time = None\n        self.attractive_results = []\n\n        self.generator = CandidateGenerator(scorer, batch_size)\n\n\n        # Initialize with the initial solution\n        self.current_text = initial_text\n        self.current_score = 100000 #self.scorer.get_perplexity(initial_text, 1)\n        self.best_text = self.current_text\n        self.best_score = self.current_score\n\n        # Maintain a history of scores for adaptive cooling\n        self.score_history = deque(maxlen=300)\n\n    def adaptive_cooling(self):\n        \"\"\"\n        Adjusts the cooling rate dynamically based on score history.\n        \"\"\"\n        if len(self.score_history) < 2:\n            return\n        volatility = max(self.score_history) - min(self.score_history)\n        if volatility < 0.05:  # Low volatility, cool down faster\n            self.temperature *= self.cooling_rate * 1.03\n        else:  # High volatility, cool down slower\n            self.temperature *= self.cooling_rate * 0.97\n\n    def optimize(self):\n        \"\"\"\n        Runs the Enhanced Simulated Annealing algorithm with batched neighbor evaluation.\n        :return: The optimized text and its perplexity score.\n        \"\"\"\n        self.start_time = time.time()\n        iteration = 0\n        stagnation_limit = 1000\n        pre_score = 100000\n        results = []\n\n        no_improvement_iterations = 0\n        results_file = \"ress.csv\"\n\n\n        while iteration < self.max_iterations:\n            # Stop if time limit is exceeded\n            if time.time() - self.start_time > self.max_time:\n                break\n\n            # Generate a batch of candidate solutions\n            try:\n              candidates_with_operations = self.generator.targeted_modify_text_batch(self.current_text, self.batch_size)\n              candidates, candidate_operations = zip(*candidates_with_operations)\n            except:\n                pass\n\n\n\n            # Evaluate perplexity scores for the batch\n           # print(\"Cn size: \", len(candidates))\n            #prefix = \"from \" #the the of and and to in is you that it with as from and have not\n            #postfix = \" we with not you have bake candy card game chocolate cookie doll eggnog fireplace fruitcake gingerbread holly mistletoe nutcracker ornament ornament peppermint poinsettia puzzle scrooge snowglobe stocking toy unwrap wrapping paper wreath yuletide advent angel beard believe bow carol candle cheer cheer chimney chimney decorations dream drive eat elf family fireplace gifts give greeting grinch holiday hohoho hope jingle jump joy kaggle laugh magi merry milk naughty nice night night peace polar reindeer relax season sing sleigh sleep star visit walk wish wonder workshop workshop\"\n            #pre = \"from the the the of of and and and to is as in that it we with not you have \"\n            #post = \" advent angel beard believe bow candle carol cheer cheer chimney chimney decorations dream drive eat elf family fireplace gifts give greeting grinch holiday hohoho hope jingle jump joy kaggle laugh magi merry milk naughty nice night night peace polar reindeer relax season sing sleigh sleep star visit walk wish wonder workshop workshop\"\n\n            #candidates_with_prefix = [pre + candidate+post for candidate in candidates]\n            candidates_with_prefix = candidates\n            candidate_scores = self.scorer.get_perplexity(candidates_with_prefix, 80) # this number changes based on sample length and GPU\n            #print(\"done\")\n\n            # Track operation performance\n            operation_scores = {op: float('inf') for op in self.generator.operations}\n            for op, score in zip(candidate_operations, candidate_scores):\n                operation_scores[op] = min(operation_scores[op], score)\n\n            # Update operation weights\n            self.generator.update_weights(operation_scores)\n\n            # Select the best candidate\n            best_idx = min(range(len(candidate_scores)), key=lambda i: candidate_scores[i])\n            new_text, new_score = candidates[best_idx], candidate_scores[best_idx]\n\n\n            # Adaptive acceptance\n            if new_score <= self.current_score or random.random() < math.exp((self.current_score - new_score) / self.temperature):\n                self.current_text = new_text\n                self.current_score = new_score\n\n                # Update the best solution if applicable\n                if new_score < self.best_score or (new_score == self.best_score and self.best_text != new_text):\n                    if (new_score == self.best_score and self.best_text != new_text):\n                      print(\"Please note this is a new text with best_score !\")\n\n                    self.best_text = new_text\n                    self.best_score = new_score\n                    print(f\"Iteration {iteration}: New Best Score: {self.best_score}, Text: {self.best_text}\")\n\n                    new_result = {\n                        \"Iteration\": iteration,\n                        \"Current_Score\": self.current_score,\n                        \"Current_Text\": self.current_text,\n                        }\n                    results.append(new_result)\n                    pd.DataFrame(results).to_csv(results_file, index=False)\n                    no_improvement_iterations = 0\n                    # stagnation_limit = stagnation_limit - 100\n                    # if stagnation_limit < 100:\n                    #   stagnation_limit = 100\n\n            # Update score history and apply adaptive cooling\n            self.score_history.append(self.current_score)\n            self.adaptive_cooling()\n\n            # Handle stagnation by restarting weights\n            if no_improvement_iterations >= stagnation_limit:\n                self.generator.operation_weights = [1.0] * len(self.generator.operations)  # Reset weights\n                no_improvement_iterations = 0\n                words = self.best_text.split()\n                # Here we tried diffrent ways to select new seed when no improvments happens\n                #mid = len(words)  // random.randint(2, 10)\n                #shuffled_part = random.sample(words[mid:], len(words[mid:]))\n                #self.current_text = ' '.join(words[:mid] + shuffled_part)\n                #shuffled_part =  random.sample(words[:mid], len(words[:mid]))\n                #self.current_text = ' '.join(shuffled_part + words[mid:])\n                self.current_text = self.best_text\n                #self.current_text = reverse_words_order(self.best_text) #' '.join(shuffled_part+words[mid:])\n                self.current_score = self.scorer.get_perplexity(self.current_text,1)\n                no_improvement_iterations = 0  # Reset stagnation counter\n                self.temperature = self.initial_temperature\n                #stagnation_limit = stagnation_limit + 100\n                pre_score = 100000\n\n            iteration += 1\n            new_result = {\n                        \"Iteration\": iteration,\n                        \"Current_Score\": self.current_score,\n                        \"Current_Text\": self.current_text,\n                        }\n            results.append(new_result)\n            pd.DataFrame(results).to_csv(results_file, index=False)\n\n\n            if iteration % 10 == 0:\n                if self.current_score < pre_score:\n                  no_improvement_iterations = 0\n                  pre_score = self.current_score\n                else:\n                  no_improvement_iterations += 10\n                print(f\"Iteration {iteration}: Best Score: {self.best_score}, Current Score and new: {self.current_score} {new_score}, Temperature: {self.temperature} {no_improvement_iterations} {len(candidates)}\")\n                new_result = {\n                    \"Iteration\": iteration,\n                    \"Current_Score\": self.current_score,\n                    \"Current_Text\": self.current_text,\n                  }\n                results.append(new_result)\n                pd.DataFrame(results).to_csv(results_file, index=False)\n\n\n\n        return self.best_text, self.best_score, self.attractive_results\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T16:56:56.533823Z","iopub.execute_input":"2025-01-31T16:56:56.534023Z","iopub.status.idle":"2025-01-31T16:56:56.554706Z","shell.execute_reply.started":"2025-01-31T16:56:56.534006Z","shell.execute_reply":"2025-01-31T16:56:56.554031Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initial Text \n\nWhat played a good role in achieving better scores was the initial text that we used to start with, after different experiments we found that the following worked best : (Sample IDs from 0 to 5)\n\n- Sorting the text alphabetically : Worked Best with Sample2\n- Stopwords sorted then alphabetically sort other words: Gave good starting point for Samples 3 and 4\n- Random Starting point (running the code many times in parallel): gave faster convergence and the better result for sample 3\n- **Sorted Stopwords, Sorted verbs, Sorted Other words** : Gave best-starting points for Sample 3 and 4 and the most important sample 5 ( even if the starting score is not as low as all the text sorted)","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\n# Load stop words (you might need to download the NLTK stopwords if you haven't already)\nimport nltk\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n# Sort stop words and other words separately\ndef custom_sort(text):\n    words = text.split(\" \")\n    stop_words_in_text = sorted([word for word in words if word.lower() in stop_words])\n    other_words = sorted([word for word in words if word.lower() not in stop_words])\n    return \" \".join(stop_words_in_text + other_words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:03:59.672075Z","iopub.execute_input":"2025-01-31T17:03:59.672375Z","iopub.status.idle":"2025-01-31T17:03:59.780971Z","shell.execute_reply.started":"2025-01-31T17:03:59.672351Z","shell.execute_reply":"2025-01-31T17:03:59.780127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"initial_temperature = 100.0  # Start with a high temperature\ncooling_rate = 0.995  # Base cooling rate\nmax_iterations = 200000  # Allow many iterations\nmax_time = 480000 # in other words do not stop :-)\nbatch_size = 5000 # Evaluate up to N candidates in parallel\n\ninitial_text = \"advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake the sleep night laugh and\"\ninitial_text = custom_sort(initial_text)\n#print(scorer.get_perplexity(initial_text,1))\n\n# optimizer = EnhancedSimulatedAnnealing(\n#         scorer,\n#         initial_text,\n#         initial_temperature,\n#         cooling_rate,\n#         max_iterations,\n#         max_time,\n#         batch_size=batch_size,\n#     )\n# best_text, best_score, best_alternatives = optimizer.optimize()\n\n# #the code really didn't reach here (with high max_time)\n# print(f\"\\nOptimized Text: {best_text}\")\n# print(f\"Best Perplexity Score: {best_score}\")\n# print(f\"Best Alternatives: {best_alternatives}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:04:06.629284Z","iopub.execute_input":"2025-01-31T17:04:06.629607Z","iopub.status.idle":"2025-01-31T17:04:06.779003Z","shell.execute_reply.started":"2025-01-31T17:04:06.629585Z","shell.execute_reply":"2025-01-31T17:04:06.778084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sol = pd.read_csv(\"/kaggle/input/santa2024-3rd-sol/sol_3rd.csv\")\nsol.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:04:13.443326Z","iopub.execute_input":"2025-01-31T17:04:13.443646Z","iopub.status.idle":"2025-01-31T17:04:13.454003Z","shell.execute_reply.started":"2025-01-31T17:04:13.443619Z","shell.execute_reply":"2025-01-31T17:04:13.45307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}