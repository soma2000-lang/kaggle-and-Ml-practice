{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70089,"databundleVersionId":9515283,"sourceType":"competition"},{"sourceId":9341631,"sourceType":"datasetVersion","datasetId":5661348},{"sourceId":9554752,"sourceType":"datasetVersion","datasetId":5737664},{"sourceId":9699155,"sourceType":"datasetVersion","datasetId":5664131},{"sourceId":9832998,"sourceType":"datasetVersion","datasetId":6031095}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport polars as pl\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold,StratifiedGroupKFold\nimport lightgbm as lgb\nfrom lightgbm import early_stopping, log_evaluation\nimport re\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nimport joblib\n\nimport kaggle_evaluation.mcts_inference_server\n\nimport pickle\n\nwith open('/kaggle/input/mcts-data/winrate_dict_updated.pkl', 'rb') as f:##\n    winrate_dict = pickle.load(f)\n    \nwith open('/kaggle/input/mcts-data/win_ruleset_dict.pkl', 'rb') as f:\n    win_rule_dict = pickle.load(f)\n    \nwith open('/kaggle/input/mcts-data/good_features.pkl', 'rb') as f:\n    good_features = pickle.load(f)\n\ndeberta_oof_df = pd.read_csv(\"/kaggle/input/deberta-mcts-models/valid_df_v1.csv\")\nruleset_pred_dict = dict(zip(deberta_oof_df['GameRulesetName'], deberta_oof_df['pred']))\n\nutility_diff_pairs = pd.read_csv(\"/kaggle/input/mcts-data/performance_diff_reduced.csv\")\nutility_diff_dict = dict(zip(utility_diff_pairs['sorted_pair'], utility_diff_pairs['performance_difference']))#","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:31:44.319982Z","iopub.execute_input":"2024-12-07T07:31:44.320313Z","iopub.status.idle":"2024-12-07T07:31:48.405498Z","shell.execute_reply.started":"2024-12-07T07:31:44.320285Z","shell.execute_reply":"2024-12-07T07:31:48.404526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns_to_drop = ['HexShape','MorrisTiling','AlquerqueTiling','TriangleTiling','ConcentricTiling','PolygonShape',\n                   'SquareTiling','SquareShape','RegularShape','TriangleShape','HexShape','SquarePyramidalShape']\n\nconstant_cols = pd.read_csv('/kaggle/input/um-gps-of-mcts-variants-constant-columns/constant_columns.csv').columns.to_list()\ntarget_col = 'utility_agent1'\n\nagent_cols = ['agent1', 'agent2','p1_selection','p1_exploration','p1_playout','p1_bounds','p2_selection'\n              ,'p2_exploration','p2_playout','p2_bounds']\ngood_features_reduced = [string for string in good_features if string not in agent_cols]\ngood_features = [string for string in good_features if string not in columns_to_drop]\n\nextra_cols = ['ARI','CLRI','McAlpine_EFLAW']\nmore_cols = ['oof_preds','deberta_pred','winrate','winrate_2']\n\ncolumns_to_analyze = [\n    'ScoreDifferenceAverage', 'ScoreDifferenceMedian', 'ScoreDifferenceMaximum',\n    'ScoreDifferenceVariance', 'ScoreDifferenceChangeAverage', 'ScoreDifferenceChangeLineBestFit',\n    'ScoreDifferenceMaxIncrease', 'ScoreDifferenceMaxDecrease'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:31:48.407459Z","iopub.execute_input":"2024-12-07T07:31:48.408103Z","iopub.status.idle":"2024-12-07T07:31:48.540277Z","shell.execute_reply.started":"2024-12-07T07:31:48.408062Z","shell.execute_reply":"2024-12-07T07:31:48.539539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ARI(txt):\n    characters=len(txt)\n    words=len(re.split(' |\\\\n|\\\\.|\\\\?|\\\\!|\\,',txt))\n    sentence=len(re.split('\\\\.|\\\\?|\\\\!',txt))\n    ari_score=4.71*(characters/words)+0.5*(words/sentence)-21.43\n    return ari_score\n\ndef McAlpine_EFLAW(txt):\n    W=len(re.split(' |\\\\n|\\\\.|\\\\?|\\\\!|\\,',txt))\n    S=len(re.split('\\\\.|\\\\?|\\\\!',txt))\n    mcalpine_eflaw_score=(W+S*W)/S\n    return mcalpine_eflaw_score\n\ndef CLRI(txt):\n    characters=len(txt)\n    words=len(re.split(' |\\\\n|\\\\.|\\\\?|\\\\!|\\,',txt))\n    sentence=len(re.split('\\\\.|\\\\?|\\\\!',txt))\n    L=100*characters/words\n    S=100*sentence/words\n    clri_score=0.0588*L-0.296*S-15.8\n    return clri_score\n\ndef preprocess_data(df_polars):\n    df_polars = df_polars.drop(constant_cols)\n    df = df_polars.with_columns(\n        pl.col('agent1').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 1).alias('p1_selection'),\n        pl.col('agent1').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 2).alias('p1_exploration').cast(pl.Float32),\n        pl.col('agent1').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 3).alias('p1_playout'),\n        pl.col('agent1').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 4).alias('p1_bounds'),\n        pl.col('agent2').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 1).alias('p2_selection'),\n        pl.col('agent2').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 2).alias('p2_exploration').cast(pl.Float32),\n        pl.col('agent2').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 3).alias('p2_playout'),\n        pl.col('agent2').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 4).alias('p2_bounds')\n    ).to_pandas()\n\n    target = df['utility_agent1']\n\n    groups = df['GameRulesetName']\n    \n    df['winrate'] = df['agent1'].map(winrate_dict)\n    df['winrate_2'] = df['agent2'].map(winrate_dict)\n\n    derberta_preds = df['GameRulesetName'].map(ruleset_pred_dict)\n    \n    df['ARI'] = df['EnglishRules'].apply(ARI)\n    df['McAlpine_EFLAW'] = df['EnglishRules'].apply(McAlpine_EFLAW)\n    df['CLRI'] = df['EnglishRules'].apply(CLRI)\n    \n    df['PlayoutsPerSecond']=df['PlayoutsPerSecond'].clip(0,25000)\n    df['MovesPerSecond']=df['MovesPerSecond'].clip(0,1000000)\n    \n    df['pair'] = df['agent1'] + df['agent2']\n\n    df[columns_to_analyze] = df[columns_to_analyze].apply(lambda x: np.log1p(x - x.min() + 1))\n    df['agent1']= df.agent1.str.rsplit('-', n=1).str[0]\n    df['agent2']= df.agent2.str.rsplit('-', n=1).str[0]\n    df['sorted_pair'] = df.apply(lambda x: '_'.join(sorted([x['agent1'], x['agent2']])), axis=1)\n    df['sorted_win_ratio'] = df['sorted_pair'].map(utility_diff_dict)\n\n    cat_features = df.columns[df.dtypes == object]\n    \n    for feature in cat_features:\n        dtype = pd.CategoricalDtype(categories=list(set(df[feature])))\n        df[feature] = df[feature].astype(dtype)\n     \n    cat_features = cat_features.tolist()\n    \n    cat_features = [string for string in cat_features if string in good_features]\n    \n    return df, target, groups,derberta_preds,cat_features\n\ndef preprocess_test(df_polars):\n    df_polars = df_polars.drop(constant_cols)\n    \n    df = df_polars.with_columns(\n        pl.col('agent1').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 1).alias('p1_selection'),\n        pl.col('agent1').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 2).alias('p1_exploration').cast(pl.Float32),\n        pl.col('agent1').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 3).alias('p1_playout'),\n        pl.col('agent1').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 4).alias('p1_bounds'),\n        pl.col('agent2').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 1).alias('p2_selection'),\n        pl.col('agent2').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 2).alias('p2_exploration').cast(pl.Float32),\n        pl.col('agent2').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 3).alias('p2_playout'),\n        pl.col('agent2').str.extract(r'MCTS-(.*)-(.*)-(.*)-(.*)', 4).alias('p2_bounds')\n    ).to_pandas()\n    \n    df['winrate'] = df['agent1'].map(winrate_dict)\n    df['winrate_2'] = df['agent2'].map(winrate_dict)\n\n    df['ARI'] = df['EnglishRules'].apply(ARI)\n    df['McAlpine_EFLAW'] = df['EnglishRules'].apply(McAlpine_EFLAW)\n    df['CLRI'] = df['EnglishRules'].apply(CLRI)\n    \n    df['PlayoutsPerSecond']=df['PlayoutsPerSecond'].clip(0,25000)\n    df['MovesPerSecond']=df['MovesPerSecond'].clip(0,1000000)\n    \n    \n    df['pair'] = df['agent1'] + df['agent2']\n\n    df[columns_to_analyze] = df[columns_to_analyze].apply(lambda x: np.log1p(x - x.min() + 1))\n\n    df.drop(columns=['GameRulesetName', 'EnglishRules', 'LudRules'], inplace=True)\n\n    cat_features = df.columns[df.dtypes == object]\n    for feature in cat_features:\n        dtype = pd.CategoricalDtype(categories=list(set(df[feature])))\n        df[feature] = df[feature].astype(dtype)\n\n    return df\n\ndef infer_lgb(data, models):\n    return np.mean([model.predict(data) for model in models], axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:31:48.541695Z","iopub.execute_input":"2024-12-07T07:31:48.542036Z","iopub.status.idle":"2024-12-07T07:31:48.562402Z","shell.execute_reply.started":"2024-12-07T07:31:48.542007Z","shell.execute_reply":"2024-12-07T07:31:48.561596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Isnt used here, its just the training function for the main model\ndef train(data, target, groups, cat_features):\n    X = data[good_features + extra_cols + more_cols]\n    y = target\n    \n    y_int = data['pair']\n    \n    cv = StratifiedGroupKFold(n_splits=Config.n_splits,random_state=2024,shuffle=True)\n    \n    models = []\n    best_scores = []\n    oof_preds = np.zeros(len(data))\n\n    for fi, (train_idx, valid_idx) in enumerate(cv.split(X,y_int, groups)):\n        print(f'Fold {fi+1}/{Config.n_splits} ...')\n        model = CatBoostRegressor(**cb_params, \n                                  cat_features=data[good_features+ more_cols].columns[data[good_features+ more_cols].dtypes == 'category'].values)\n        \n        model.fit(X.iloc[train_idx], y.iloc[train_idx],\n                  eval_set=[(X.iloc[valid_idx], y.iloc[valid_idx])],\n                  early_stopping_rounds=100)\n        \n        models.append(model)\n        \n        best_validation_score = model.best_score_['validation']['RMSE']\n        best_scores.append(best_validation_score)\n        print(f\"Best validation score for fold {fi+1}: {best_validation_score}\")\n        \n        oof_preds[valid_idx] = model.predict(X.iloc[valid_idx])\n\n    oof_df = pd.DataFrame({\n        'true_values': y,\n        'oof_predictions': oof_preds\n    })\n\n    avg_score = sum(best_scores) / len(best_scores)\n    print(f'Average validation score across {Config.n_splits} folds: {avg_score}')\n    \n    return models,oof_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:31:48.56465Z","iopub.execute_input":"2024-12-07T07:31:48.565036Z","iopub.status.idle":"2024-12-07T07:31:48.577825Z","shell.execute_reply.started":"2024-12-07T07:31:48.564995Z","shell.execute_reply":"2024-12-07T07:31:48.57681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\nVER=1\nLOAD_FROM = '/kaggle/input/deberta-mcts-models/'\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset\nimport string\n\nclass Tokenize(object):\n    def __init__(self, valid, tokenizer):\n        self.tokenizer = tokenizer\n        self.valid = valid\n        \n    def get_dataset(self, df):\n        ds = Dataset.from_dict({\n            'essay_id': df.index.to_list(),\n            'full_text': df['LudRules'].tolist(),\n        })\n        return ds\n        \n    def tokenize_function(self, example):\n        return self.tokenizer(\n            example['full_text'], truncation=True, max_length=3072#CFG.max_length###\n        )\n    \n    def __call__(self):\n        valid_ds = self.get_dataset(self.valid)\n        tokenized_valid = valid_ds.map(\n            self.tokenize_function, batched=True)\n        \n        return tokenized_valid, self.tokenizer\n\ntraining_args = TrainingArguments(\".\", per_device_eval_batch_size=1, report_to=\"none\")\n\ntokenizer = AutoTokenizer.from_pretrained(LOAD_FROM + f'deberta-v3-small_MCTS_fold_0_v{VER}')\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\nmodel = AutoModelForSequenceClassification.from_pretrained(LOAD_FROM + f'deberta-v3-small_MCTS_fold_0_v{VER}')\nmodel_2 = AutoModelForSequenceClassification.from_pretrained(LOAD_FROM + f'deberta-v3-small_MCTS_fold_1_v{VER}')\nmodel_3 = AutoModelForSequenceClassification.from_pretrained(LOAD_FROM + f'deberta-v3-small_MCTS_fold_2_v{VER}')\nmodel_4 = AutoModelForSequenceClassification.from_pretrained(LOAD_FROM + f'deberta-v3-small_MCTS_fold_3_v{VER}')\n\ndef remove_punctuation(text):\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\ndef extract_end_section(text):\n    end_idx = text.rfind(\"(end\")\n    \n    if end_idx != -1:\n        return text[end_idx:]\n    else:\n        return tex","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:31:48.579162Z","iopub.execute_input":"2024-12-07T07:31:48.579553Z","iopub.status.idle":"2024-12-07T07:32:08.230675Z","shell.execute_reply.started":"2024-12-07T07:31:48.579514Z","shell.execute_reply":"2024-12-07T07:32:08.229727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run_i = 0\n\ndef predict(test_data, submission):\n    global run_i, models, pre_models, oof_df,tokenizer,model,TRAIN_MODELS,seeds,LOAD_PRETRAIN\n    if run_i == 0:\n        pre_models = joblib.load('/kaggle/input/mcts-pretrain-data/pre_models_array.pkl')\n        oof_df = pd.read_csv('/kaggle/input/mcts-pretrain-data/oof_predictions.csv')\n        models = joblib.load('/kaggle/input/mcts-data/models_array.pkl')\n                     \n    run_i += 1\n    \n    test_data = test_data.to_pandas()\n    test_data['LudRules'] = test_data['LudRules'].apply(extract_end_section)\n    test_data['LudRules'] = test_data['LudRules'].apply(remove_punctuation)\n    tokenize = Tokenize(test_data, tokenizer)\n    tokenized_test, _ = tokenize()\n    def infer_with_trainer(model):\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_test,\n            data_collator=data_collator,\n            tokenizer=tokenizer,\n        )\n        return trainer.predict(tokenized_test).predictions\n    \n    predictions = infer_with_trainer(model)\n    predictions_2 = infer_with_trainer(model_2)\n    predictions_3 = infer_with_trainer(model_3)\n    predictions_4 = infer_with_trainer(model_4)\n    test_data = pl.from_pandas(test_data)\n    \n    test_data = preprocess_test(test_data)\n    test_data['oof_preds'] = infer_lgb(test_data[good_features_reduced+extra_cols], pre_models)\n    test_data['deberta_pred'] = predictions*0.25+predictions_2*0.25+predictions_3*0.25+predictions_4*0.25\n    prediction_cat = infer_lgb(test_data[good_features + extra_cols + more_cols], models)\n    clipped_predictions = np.clip(prediction_cat*1.15, -1.0, 1.0)\n\n    # Return the submission with clipped predictions\n    return submission.with_columns(pl.Series(target_col, clipped_predictions ))#discrete_predictions\n\ninference_server = kaggle_evaluation.mcts_inference_server.MCTSInferenceServer(predict)\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv',\n            '/kaggle/input/um-game-playing-strength-of-mcts-variants/sample_submission.csv'\n        )\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:32:08.231849Z","iopub.execute_input":"2024-12-07T07:32:08.232538Z","iopub.status.idle":"2024-12-07T07:32:21.311482Z","shell.execute_reply.started":"2024-12-07T07:32:08.23251Z","shell.execute_reply":"2024-12-07T07:32:21.310504Z"}},"outputs":[],"execution_count":null}]}